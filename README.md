

This is a repository for organizing articles related to Domain generalization, OOD, optimization, data-centric learning, prompt learning, robutness, and causality. Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.

### About Me :high_brightness: 
I'm the second year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/), alibaba DAMO Academy, work with Prof. [Rong Jin](https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN).


###  ğŸ”¥ Updated 2023-5-26
- Recent Domain generalization, domain adaptation papers on **ICLR 2023**, **CVPR 2023** and **Arxiv** have been updated.
- Our paper  [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661) has been accepted by **SIGKDD 2023**. [[Code]](https://github.com/yfzhang114/AdaNPC) [[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
- Our paper  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566) has been accepted by **ICML 2023**. [[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
- Our paper [Free Lunch for Domain Adversarial Training: Environment Label Smoothing](https://arxiv.org/abs/2302.00194) has been accepted by **ICLR 2023**. [[Code]](https://github.com/yfzhang114/Environment-Label-Smoothing)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/600466715)
- Our paper [Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2202.01336) has been accepted by **NeurIPS ML Safety** workshop. [[Code]](https://github.com/hlzhang109/TransTEE)
- Our paper Towards Principled Disentanglement for Domain Generalization has been selected for an CVPR **ORAL** presentation. :blush: [[Reading Notes]](https://zhuanlan.zhihu.com/p/477855079) [[Code]](https://github.com/hlzhang109/DDG)  [[paper]](https://arxiv.org/abs/2111.13839)
- Papers about test-time adaptation methods have been updated.

# Table of Contents (ongoing)
* [Generalization/OOD](#generalizationood)
   * [2023](#2023)
   * [2022](#2022)
   * [2021](#2021)
   * [2020](#2020)
   * [OLD but Important](#old-but-important)
   * [Survey](#survey)
* [Test-time adaptation](#test-time-adaptation)
* [Robutness/Adaptation/Fairness/OOD Detection](#robutnessadaptationfairnessood-detection)
   * [2022](#2022-1)
   * [2021](#2021-1)
   * [Before 2021](#before-2021)
* [Causality](#causality)
   * [Individual Treatment Estimation](#individual-treatment-estimation)
* [Data-Centric/Prompt](#data-centricprompt)
   * [Data Centric](#data-centric)
   * [Prompts](#prompts)
* [Optimization/GNN/Energy/Others](#optimizationgnnenergygenerativeothers)
   * [Optimization](#optimization)
   * [LTH (Lottery Ticket Hypothesis)](#lth-lottery-ticket-hypothesis)
   * [Generative Model (Mainly Diffusion Model)](#generative-model-mainly-diffusion-model)
   * [Implicit Neural Representation (INR)](#implicit-neural-representation-inr)
   * [Survey](#survey-1)
# Generalization/OOD

## 2023
0. ICLR [Free Lunch for Domain Adversarial Training: Environment Label Smoothing](https://arxiv.org/abs/2302.00194)(ç¯å¢ƒæ ‡ç­¾å¹³æ»‘ï¼Œä¸€è¡Œä»£ç æå‡å¯¹æŠ—å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ³›åŒ–æ€§). [[Code]](https://github.com/yfzhang114/Environment-Label-Smoothing)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/600466715)
1. ICLR [Out-of-Distribution Representation Learning for Time Series Classification](https://arxiv.org/abs/2209.07027)(ä»OODçš„è§’åº¦è€ƒè™‘æ—¶åºåˆ†ç±»çš„é—®é¢˜)
2. ICLR [Contrastive Learning for Unsupervised Domain Adaptation of Time Series](https://arxiv.org/abs/2206.06243)(ç”¨å¯¹æ¯”å­¦ä¹ å¯¹å…¶ç±»é—´åˆ†å¸ƒä¸ºæ—¶åºDAå­¦ä¸€ä¸ªå¥½çš„è¡¨å¾)
3. ICLR [Pareto Invarian Risk Minimization](https://openreview.net/forum?id=esFxSb_0pSL)(é€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–è§’åº¦ç†è§£ä¸ç¼“è§£OOD/DGä¼˜åŒ–éš¾é—®é¢˜)
4. ICLR [Fairness and Accuracy under Domain Generalization](https://arxiv.org/abs/2301.13323)(ä¸ä»…è€ƒè™‘æ³›åŒ–çš„æ€§èƒ½ï¼Œä¹Ÿè€ƒè™‘æ³›åŒ–çš„å…¬å¹³æ€§)
6. Arxiv [Adversarial Style Augmentation for Domain Generalization](https://arxiv.org/abs/2301.12643)(å¯¹æŠ—å­¦ä¹ æ·»åŠ å›¾åƒæ‰°åŠ¨ä»¥æå‡æ¨¡å‹æ³›åŒ–æ€§)
7. Arxiv [CLIPood: Generalizing CLIP to Out-of-Distributions](https://arxiv.org/abs/2302.00864)(ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œå…‹æœdomain shift and open classä¸¤ä¸ªé—®é¢˜)
8. ICML  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566)(ç”¨KNNè¿›è¡Œæµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œä»ç†è®ºä¸Šåˆ†æäº†TTA workçš„åŸå› )[[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
9. SIGKDD [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661)(æ¯ä¸ªåŸŸå­¦ä¹ å•ç‹¬çš„åˆ†ç±»å™¨ï¼Œæµ‹è¯•é˜¶æ®µæ ¹æ®entropyåŠ¨æ€ç»„åˆ)[[Code]](https://github.com/yfzhang114/AdaNPC)[[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
10. CVPR [Federated Domain Generalization with Generalization Adjustment](https://scholar.google.com/scholar_url?url=https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf&hl=zh-CN&sa=X&d=13348506996942284912&ei=sTpvZIjhI9OQ6rQP29uDqAU&scisig=AGlGAw8T1YjQNN8nVv2lI6LPBiGS&oi=scholaralrt&hist=lUnt8X4AAAAJ:7797965790415635509:AGlGAw-zJ0qtstLHlwZtiYmf7uNN&html=&pos=1&folt=rel)(ä¸ºè”é‚¦åŸŸæ³›åŒ–(FedDG)æä¾›äº†ä¸€ä¸ªæ–°çš„æ–°çš„å‡å°æ–¹å·®çš„æ­£åˆ™é¡¹ä»¥é¼“åŠ±å…¬å¹³æ€§)
11. CVPR [Distribution Shift Inversion for Out-of-Distribution Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf)(TTAæ–¹æ³•ï¼Œå°†OoDæµ‹è¯•æ ·æœ¬ç”¨ä»…åœ¨æºåˆ†å¸ƒä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å‘è®­ç»ƒåˆ†å¸ƒè½¬ç§»ç„¶åå†æµ‹è¯•)
12. CVPR [SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization](https://arxiv.org/abs/2305.11615)(é€šè¿‡ç§»é™¤é‚£äº›å¼ºçƒˆä¾èµ–å·²è¯†åˆ«çš„è™šå‡ç‰¹å¾çš„ç½‘ç»œåˆ†æ”¯æ¥å®ç°modular risk minimization (MRM))
13. CVPR [Improved Test-Time Adaptation for Domain Generalization](https://arxiv.org/abs/2304.04494)(ä½¿ç”¨ä¸€ä¸ªå…·æœ‰å¯å­¦ä¹ å‚æ•°çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯é¢„å®šä¹‰çš„å‡½æ•°)

## 2022

0. CVPR Oral [Towards Principled Disentanglement for Domain Generalization](https://zhuanlan.zhihu.com/p/477855079)(å°†è§£è€¦ç”¨äºDGï¼Œæ–°ç†è®ºï¼Œæ–°æ–¹æ³•)
1. Arxiv [How robust are pre-trained models to distribution shift?](https://arxiv.org/abs/2206.08871)(è‡ªç›‘ç£æ¨¡å‹æ¯”æœ‰ç›‘ç£ä»¥åŠæ— ç›‘ç£æ¨¡å‹æ›´é²æ£’ï¼Œåœ¨å°éƒ¨åˆ†OODæ•°æ®ä¸Šé‡æ–°è®­ç»ƒclassifieræå‡å¾ˆå¤§)
2. ICML [A Closer Look at Smoothness in Domain Adversarial Training](https://arxiv.org/abs/2206.08213)(å¹³æ»‘åˆ†ç±»æŸå¤±å¯ä»¥æé«˜åŸŸå¯¹æŠ—è®­ç»ƒçš„æ³›åŒ–æ€§èƒ½)
3. CVPR [Bayesian Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/528829486)(ç¼“è§£IRMåœ¨æ¨¡å‹è¿‡æ‹Ÿåˆæ—¶é€€åŒ–ä¸ºERMçš„é—®é¢˜)
4. CVPR [Towards Unsupervised Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(å…³æ³¨æ¨¡å‹é¢„è®­ç»ƒçš„è¿‡ç¨‹å¯¹DGä»»åŠ¡çš„å½±å“ï¼Œè®¾è®¡äº†ä¸€ä¸ªåœ¨DGæ•°æ®é›†æ— ç›‘ç£é¢„è®­ç»ƒçš„ç®—æ³•)
5. CVPR [PCL: Proxy-based Contrastive Learning for Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(ç›´æ¥é‡‡ç”¨æœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ç”¨äºDGæ•ˆæœå¹¶ä¸å¥½ï¼Œæœ¬æ–‡æå‡ºå¯è¡Œæ–¹æ³•)
6. CVPR [Style Neophile: Constantly Seeking Novel Styles for Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å¤šé£æ ¼çš„æ•°æ®)
7. Arxiv [WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks](https://woods-benchmarks.github.io/)(ä¸€ä¸ªå…³äºæ—¶åºæ•°æ®OODçš„å¤šä¸ªbenchmark)
8. Arxiv [A Broad Study of Pre-training for Domain Generalization and Adaptation](https://arxiv.org/pdf/2203.11819.pdf)(æ·±å…¥ç ”ç©¶äº†é¢„è®­ç»ƒå¯¹äºDA,DGä»»åŠ¡çš„ä½œç”¨ï¼Œç®€å•çš„ä½¿ç”¨ç›®å‰æœ€å¥½çš„backboneè¶³å·²å–å¾—SOTAçš„æ•ˆæœ)
9. Arxiv [Domain Generalization by Mutual-Information Regularization with Pre-trained Models](https://arxiv.org/pdf/2203.10789.pdf)(ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æŒ‡å¯¼finetuneçš„è¿‡ç¨‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›)
10. ICLR Oral [A Fine-Grained Analysis on Distribution Shift](https://zhuanlan.zhihu.com/p/466675818)(å¦‚ä½•å‡†ç¡®çš„å®šä¹‰distribution shiftï¼Œä»¥åŠå¦‚ä½•ç³»ç»Ÿçš„æµ‹é‡æ¨¡å‹çš„é²æ£’æ€§)
11. ICLR Oral [Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution](https://zhuanlan.zhihu.com/p/466675818)(fine-tuningï¼ˆå¾®è°ƒï¼‰å’Œlinear probingç›¸è¾…ç›¸æˆ)
12. ICLR Spotlight [Towards a Unified View of Parameter-Efficient Transfer Learning](https://zhuanlan.zhihu.com/p/466675818)(ç»Ÿä¸€çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç†è®ºæ¡†æ¶)
13. ICLR Spotlight [How Do Vision Transformers Work?](https://zhuanlan.zhihu.com/p/466675818)(Vision Transformers (ViTs)çš„ä¼˜è‰¯ç‰¹æ€§)
14. ICLR Spotlight [On Predicting Generalization using GANs](https://zhuanlan.zhihu.com/p/466675818)(ä½¿ç”¨æºåŸŸæ•°æ®è®­ç»ƒå‡ºçš„GANæ¥é¢„æµ‹æµ‹è¯•è¯¯å·®)
15. ICLR Poster [Uncertainty Modeling for Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/466675818)(åŸŸæ³›åŒ–æ—¶è€ƒè™‘ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•)
16. ICLR Poster [Gradient Matching for Domain Generalization](https://zhuanlan.zhihu.com/p/466675818)(é¼“åŠ±æ¥è‡ªä¸åŒåŸŸçš„æ¢¯åº¦ä¹‹é—´çš„å†…ç§¯æ›´å¤§)
17. ICML [DNA: Domain Generalization with Diversified Neural Averaging](https://zhuanlan.zhihu.com/p/553511043)(classifier ensembleï¼Œå³å¯¹åˆ†ç±»å™¨è¿›è¡Œé›†æˆã€‚æœ¬æ–‡ä»ç†è®ºå’Œå®éªŒè§’åº¦è®¨è®ºäº†ensembleä¸DGä»»åŠ¡çš„connectionã€‚)
18. ICML [Model Agnostic Sample Reweighting for Out-of-Distribution Learning](https://zhuanlan.zhihu.com/p/553511043)(bi-levelçš„å»æ‰¾ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬åŠ æƒæ–¹å¼)
19. ICML [Sparse Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/553511043)(åˆ©ç”¨å…¨å±€ç¨€ç–æ€§çº¦æ¥é˜²æ­¢ä¼ªç‰¹å¾åœ¨è®­ç»ƒè¿‡ç¨‹è¢«ä½¿ç”¨)
20. Arxiv [grounding visual representations with texts for domain generalization](http://arxiv.org/abs/2207.10285)(ç”¨è·¨æ¨¡æ€çš„æ•°æ®ä½œä¸ºæ¨¡å‹çš„ç›‘ç£ä¿¡æ¯å¯ä»¥æå‡æ³›åŒ–æ€§)
21. Arxiv [On the Strong Correlation Between Model Invarianceand Generalization](http://arxiv.org/abs/2207.07065)(æ¨¡å‹é¢„æµ‹çš„ä¸å˜æ€§ä¸æ³›åŒ–æ€§æœ‰å¼ºç›¸å…³ï¼Œè¿™é‡Œçš„ä¸å˜æ€§æ˜¯å¯¹xä¸åŒperturbationé¢„æµ‹çš„ä¸å˜æ€§)
22. NeurIPS [Probable Domain Generalization via Quantile Risk Minimization](https://arxiv.org/abs/2207.09944)(å°†DGå»ºæ¨¡æˆæ¦‚ç‡æ³›åŒ–çš„é—®é¢˜ï¼Œæ—¢ä¸æ˜¯worst-caseï¼Œä¹Ÿä¸æ˜¯average performance)
23. NeurIPS [Improving Multi-Task Generalization via Regularizing Spurious Correlation](https://arxiv.org/abs/2205.09797)(å»é™¤å¯¹ä»»åŠ¡æ ‡ç­¾çš„è™šå‡ä¾èµ–ï¼Œä»è€Œæå‡å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆæœ)
24. NeurIPS [Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction](https://arxiv.org/abs/2206.07085)(ä»ç†è®ºä¸Šè§£é‡Šå½’ä¸€åŒ–å±‚ä½¿å¾—æŸå¤±é¢é”åº¦é™ä½ï¼ŒGDæ›´æ˜“ä¼˜åŒ–)
25. NeurIPS [Assaying Out-Of-Distribution Generalization in Transfer Learning](https://zhuanlan.zhihu.com/p/573246040)(å…¨é¢çš„å¯¹æ¨¡å‹é²æ£’æ€§çš„å®šä¹‰æå‡ºæ–°çš„è§è§£)
26. NeurIPS [On the Strong Correlation Between Model Invariance and Generalization](https://zhuanlan.zhihu.com/p/573246040)(å¯¹å¯¹æ³›åŒ–ä¸ä¸å˜æ€§ä¹‹é—´çš„å…³ç³»è¿›è¡Œå®šé‡çš„åˆ†æ)
27. NeurIPS [Ensemble of Averages: Improving Model Selectionand Boosting Performance in Domain Generalization](https://zhuanlan.zhihu.com/p/573246040)(è®­ç»ƒè¿‡ç¨‹ä¸­OODæ•°æ®æ€§èƒ½æ³¢åŠ¨å¾ˆå¤§)
28. NeurIPS [Diverse Weight Averaging for Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/573246040)(æ²¿ç€è®­ç»ƒè½¨è¿¹å¹³å‡è·å¾—çš„æƒé‡)
29. NeurIPS [Improving Out-of-Distribution Generalization byAdversarial Training with Structured Priors](https://openreview.net/forum?id=Ku1afTnmozi)(ä½¿ç”¨domain specific structured low-rank perturbationsæ¥å¯¹æŠ—å­¦ä¹ æå‡OODæ€§èƒ½)
29. NeurIPS Outstanding [On-Demand Sampling:Learning Optimally from Multiple Distributions](https://arxiv.org/pdf/2210.12529.pdf)(ä¸€ä¸ªæœ‰ç†è®ºä¿è¯çš„å¤šåŸŸå­¦ä¹ ç®—æ³•ï¼Œè¾¾åˆ°äº†ç›®å‰æœ€ä½çš„sample complexity)
30. Arxiv [On Feature Learning in the Presence of Spurious Correlations](http://arxiv.org/abs/2210.11369)(ERMå·²ç»èƒ½å¤Ÿå­¦åˆ°å¾ˆå¥½çš„ç‰¹å¾äº†)
31. Arxiv [Simulating Bandit Learning from User Feedback for Extractive Question Answering](https://arxiv.org/abs/2203.10079)(å¼•å…¥å°‘é‡human evaluationå¯ä»¥æå‡æ¨¡å‹æ³›åŒ–æ€§)
32. ICLR [Uncertainty Modeling for Out-of-Distribution Generalization](https://arxiv.org/abs/2202.03958)(æ”¹å˜å›¾è±¡å‡å€¼/æ–¹å·®æ¥åšæ•°æ®å¢å¼ºï¼Œå‡å€¼æ–¹å·®è€ƒè™‘batchä¸­çš„ä¸ç¡®å®šæ€§)

## 2021
1. ICML [Improved OOD Generalization via Adversarial Training and Pre-training](https://proceedings.mlr.press/v139/yi21a.html)(ä»ç†è®ºä¸Šè¡¨æ˜ï¼Œä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œé‚£ä¹ˆå¯¹ä¸‹æ¸¸OODæ•°æ®çš„æ³›åŒ–å¯ä»¥æä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚)
2. ICCV [CrossNorm and SelfNorm for Generalization under Distribution Shifts](https://zhuanlan.zhihu.com/p/426728622)(æ€è·¯ç®€å•çš„æ­£åˆ™åŒ–æŠ€æœ¯ç”¨äºDG)
3. ICCV [A Style and Semantic Memory Mechanism for Domain Generalization](https://zhuanlan.zhihu.com/p/426728622)(å°è¯•ç€å»ä½¿ç”¨intra-domain style invarianceæ¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½)
4. Arxiv: [Towards a Theoretical Framework of Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆæ–°ç†è®ºï¼‰
5. Arxiv(**Yoshua Bengio**) _Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization_ (å½“OODé‡åˆ°ä¿¡æ¯ç“¶é¢ˆç†è®º)
6. Arxiv _Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation_
7. Arxiv _Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation_(ä½¿ç”¨çŸ¥è¯†è’¸é¦ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µ)
8. Arxiv _Delving Deep into the Generalization of Vision Transformers under Distribution Shifts_ (è§†è§‰transformerçš„æ³›åŒ–æ€§è®¨è®º)
9. Arxiv _Training Data Subset Selection for Regression with Controlled Generalization Error_ (ä»å¤§é‡è®­ç»ƒå®ä¾‹ä¸­é€‰æ‹©æ•°æ®å­é›†ï¼Œå¹¶ä¿æŒå¯æ¯”çš„æ³›åŒ–æ€§)
10. Arxiv(**MIT**) _Measuring Generalization with Optimal Transport_ (ç½‘ç»œå¤æ‚åº¦ä¸æ³›åŒ–æ€§çš„ç†è®ºç ”ç©¶ï¼Œ)
11. Arxiv(**SJTU**) [OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms](https://view.inews.qq.com/a/20210615A04V1C00?tbkt=B1&uid=) (æ­ç¤ºOODçš„è¯„æµ‹æ ‡å‡†å°šä¸å®Œå–„å¹¶æå‡ºè¯„æµ‹æ–¹æ¡ˆ)
12. Arxiv (Tsinghu) _Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization_ (æ–°çš„taskï¼šæ— ç›‘ç£çš„DGï¼ŒæºåŸŸçš„æ•°æ®æ ‡ç­¾ä¸å¯ä»¥ç”¨)
13. ICML Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆå½©ç¥¨æ¨¡å‹å¯»æ‰¾æ¨¡å‹ä¸­æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„å°æ¨¡å‹ï¼‰
14. ICML Oralï¼š[Domain Generalization using Causal Matching](https://zhuanlan.zhihu.com/p/382608823) (contrastive lossç‰¹å¾å¯¹é½+ç‰¹å¾ä¸å˜æ€§çº¦æŸ)
15. ICML Oral: _Just Train Twice: Improving Group Robustness without Training Group Information_
16. ICML Spotlight: [Environment Inference for Invariant Learning](https://zhuanlan.zhihu.com/p/382608823) (æ²¡æœ‰åŸŸæ ‡ç­¾å¦‚ä½•å­¦ä¹ åŸŸä¸å˜æ€§ç‰¹å¾ï¼Ÿ)
17. ICLR Poster: [Understanding the failure modes of out-of-distribution generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆOODå¤±è´¥çš„ä¸¤ç§åŸå› ï¼‰
18. ICLR Poster: [An Empirical Study of Invariant Risk Minimization](https://openreview.net/forum?id=jrA5GAccy_)(å¯¹IRMçš„å®éªŒæ€§æ¢ç´¢ï¼Œå¦‚å¯è§åŸŸçš„diversityå¦‚ä½•å½±å“IRMæ€§èƒ½ç­‰)
19. ICLR Poster _In Search of Lost Domain Generalization_ (æ²¡æœ‰model selectionçš„æ–¹æ³•ä¸æ˜¯å¥½æ–¹æ³•ï¼Œå¦‚ä½•æ ¹æ®éªŒè¯é›†é€‰æ‹©æ¨¡å‹ï¼Ÿ)
20. ICLR Poster [Modeling the Second Player in Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(ç”¨å¯¹æŠ—å­¦ä¹ å»ºæ¨¡DROçš„uncertainty set)
21. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
22. ICLR Spotlight(**Yoshua Bengio**) [Systematic generalisation with group invariant predictions](https://zhuanlan.zhihu.com/p/382608823) (å°†æ¯ä¸ªç±»åˆ†æˆä¸åŒçš„domain(_environment inference_ï¼Œç„¶åçº¦æŸæ¯ä¸ªåŸŸçš„ç‰¹å¾å°½å¯èƒ½ä¸€è‡´ä»è€Œé¿å…è™šå‡ä¾èµ–))
23. CVPR Oral: [Reducing Domain Gap by Reducing Style Bias](https://zhuanlan.zhihu.com/p/382608823) (channel-wise å‡å€¼ä½œä¸ºå›¾åƒé£æ ¼ï¼Œå‡å°‘CNNå¯¹é£æ ¼çš„ä¾èµ–)
24. AISTATS _Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions_
25. AISTATS Oral _Does Invariant Risk Minimization Capture Invariance_(IRMåªæœ‰åœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„æƒ…å†µä¸‹æ‰èƒ½çœŸæ­£æ•æ‰ä¸å˜å½¢ç‰¹å¾)
26. NeurIPS [Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests](https://arxiv.org/abs/2106.00545)(æœ¬æ–‡ä½¿ç”¨å› æœå·¥å…·è®¾è®¡äº†ä¸€ä¸ªå¯è¡Œçš„ç®—æ³•ï¼Œå°†åäº‹å®æ¨ç†ä¸åŸŸæ³›åŒ–ï¼ˆOODï¼‰è”ç³»èµ·æ¥ï¼Œè¿›è¡Œæœ‰æ•ˆçš„â€œstress testâ€ï¼Œæ¯”å¦‚å˜åŒ–ä¸€ä¸ªå¥å­åŒ…å«çš„çš„genderä¿¡æ¯ï¼Œçœ‹æœ€åæƒ…æ„Ÿåˆ†ç±»ä¼šä¸ä¼šæ”¹å˜ã€‚)
27. NeurIPS [Adaptive Risk Minimization: Learning to Adapt to Domain Shift](https://zhuanlan.zhihu.com/p/357962431)(åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥æ›´å¥½åœ°å¤„ç†æ–°domainå¼•èµ·çš„distribution shift)
28. NeurIPS [An Empirical Investigation of Domain Generalization with Empirical Risk Minimizers](https://zhuanlan.zhihu.com/p/357962431)(åŸºäºdomain adaptationçš„ç†è®ºæµ‹é‡æ–¹æ³•ä¸èƒ½å‡†ç¡®åœ°æ•æ‰OODæ³›åŒ–è¡Œä¸º)
29. NeurIPS Spotlight [Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization](https://zhuanlan.zhihu.com/p/357962431)(åœ¨testçš„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°æ¨¡å‹å¤´éƒ¨çš„linearå±‚)
30. NeurIPS [Why Do Better Loss Functions Lead to Less Transferable Features?](https://zhuanlan.zhihu.com/p/357962431)(æœ¬æ–‡ç ”ç©¶äº†è®­ç»ƒç›®æ ‡çš„é€‰æ‹©å¦‚ä½•å½±å“å·ç§¯ç¥ç»ç½‘ç»œåœ¨ImageNetä¸Šè®­ç»ƒå¾—åˆ°çš„å¯è¿ç§»æ€§)

## 2020
1. Arxiv [I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models](https://zhuanlan.zhihu.com/p/288980706)(å°†Domain Adaptationçœ‹ä½œæ˜¯å› æœå›¾æ¨ç†é—®é¢˜)
2. Arxiv (**Stanford**)_Distributionally Robust Lossesfor Latent Covariate Mixtures_.
3. NeurIPS [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)(ä½¿ç”¨èƒ½é‡æ¨¡å‹æ£€æµ‹OODæ ·æœ¬)
4. NeurIPS _Fairness without demographics through adversarially reweighted learning_ (åˆ©ç”¨å¯¹æŠ—å­¦ä¹ å¯¹éš¾æ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œå¸Œæœ›åŠ æƒåçš„æ ·æœ¬ä½¿å¾—åˆ†ç±»å™¨çš„æŸå¤±æ›´å¤§)
5. NeurIPS _Self-training Avoids Using Spurious Features Under Domain Shift_ (ä½¿ç”¨target domainçš„æ— æ ‡ç­¾æ•°æ®è®­ç»ƒæœ‰åŠ©äºé¿å…ä½¿ç”¨è™šå‡ç‰¹å¾)
6. NeurIPS _What shapes feature representations? Exploring datasets, architectures, and training_(Simplicity Biasï¼Œç¥ç»ç½‘ç»œå€¾å‘äºæ‹Ÿåˆâ€œå®¹æ˜“â€çš„ç‰¹å¾)
7. Arxiv [Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (å¥ åŸºä¹‹ä½œï¼Œè·³å‡ºç»éªŒé£é™©æœ€å°åŒ–--ä¸å˜é£é™©æœ€å°åŒ–)
8. ICLR Poster [The Risks of Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (ä¸å˜é£é™©æœ€å°åŒ–çš„ç¼ºé™·:åŸŸæ•°ç›®è¿‡å°‘IRMå³å¤±è´¥)
9. ICLR _Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization_(GroupDRO: æ‹¥æœ‰å¼ºæ­£åˆ™çš„DRO)
10. ICML _An investigation of why overparameterizationexacerbates spurious correlations_(ç¥ç»ç½‘ç»œçš„è¿‡å‚æ•°åŒ–æ˜¯é€ æˆç½‘ç»œä½¿ç”¨è™šå‡ç›¸å…³æ€§çš„é‡è¦åŸå› )
11. ICML UDA workshop _Learning Robust Representations with Score Invariant Learning_(éå½’ä¸€åŒ–ç»Ÿè®¡æ¨¡å‹ï¼šç”¨èƒ½é‡å­¦ä¹ çš„æ–¹å¼åšOOD)

## OLD but Important
1. ICML 2018 Oral (**Stanford**) _Fairness Without Demographics in Repeated Loss Minimization._
2. ICCV 2017 [CCSA--Unified Deep Supervised Domain Adaptation and Generalization](https://blog.csdn.net/Adupanfei/article/details/85165667) (å¯¹æ¯”æŸå¤±å¯¹é½æºåŸŸç›®æ ‡åŸŸæ ·æœ¬ç©ºé—´)
3. JSTOR (**Peters**)Causal inference by using invariant prediction: identification and confidence intervals.
4. ICML 2015 [Towards a Learning Theory of Cause-Effect Inference](ä½¿ç”¨kernel mean embeddingå’Œåˆ†ç±»å™¨è¿›è¡Œcasual inference                  )
5. IJCAI 2020 (**CMU**) _Causal Discovery from Heterogeneous/Nonstationary Data_

## Survey
1. [Causality åŸºç¡€æ¦‚å¿µæ±‡æ€»](https://zhuanlan.zhihu.com/p/269625734)
2. [Domain AdaptationåŸºç¡€æ¦‚å¿µä¸ç›¸å…³æ–‡ç« è§£è¯»](https://zhuanlan.zhihu.com/p/272508224)

# Test-time Adaptation

0. ICML  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566)(ç”¨KNNè¿›è¡Œæµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œä»ç†è®ºä¸Šåˆ†æäº†TTA workçš„åŸå› )[[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
1. NeurIPS 2021 [Spotlight] [Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization](https://zhuanlan.zhihu.com/p/559916666)(åœ¨testçš„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°æ¨¡å‹å¤´éƒ¨çš„linearå±‚)
2. CVPR 2021 [Adaptive Methods for Real-World Domain Generalization](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶è¾“å…¥source domain embeddingï¼Œå³testæ—¶åˆ©ç”¨domainä¿¡æ¯)
3. ICLR 2021 [Spotlight] [Tent: Fully Test-Time Adaptation by Entropy Minimization](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶æœ€å°åŒ–æ¨¡å‹é¢„æµ‹çš„entropy)
4. ICCV 2021 [Test-Agnostic Long-Tailed Recognition y Test-Time Aggregating Diverse Experts with Self-Supervision](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶ä¼˜åŒ–æ ·æœ¬çš„è‡ªç›‘ç£æŸå¤±)
5. NeurIPS 2022 [Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering](https://arxiv.org/abs/2206.02721)(å‘ç°æºå’Œç›®æ ‡åŸŸä¸­çš„é›†ç¾¤ï¼Œå¹¶å°†ç›®æ ‡é›†ç¾¤ä¸æºé›†ç¾¤è¿›è¡ŒåŒ¹é…ï¼Œä»¥æ”¹è¿›æ³›åŒ–ã€‚)
6. NeurIPS 2022 [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511)(æµ‹è¯•é˜¶æ®µæ ¹æ®æœ€å°åŒ–é¢„æµ‹ç†µä»è€Œæ›´æ–°prompt)
7. NeurIPS 2022 [MEMO: Test Time Robustness via Adaptation and Augmentation](https://arxiv.org/abs/2110.09506)(æµ‹è¯•é˜¶æ®µæ•°æ®å¢å¼º+æœ€å°åŒ–ç†µ)
8. CVPR 2022 [Continual Test-Time Domain Adaptation](https://arxiv.org/abs/2203.13591)(ä»ä¸€ä¸ªæºåŸŸadaptåˆ°ä¸€ç³»åˆ—è¿ç»­æ”¹å˜çš„ç›®æ ‡åŸŸ)
9. Arxiv [A Simple Test-Time Method for Out-of-Distribution Detection](https://arxiv.org/pdf/2207.08210.pdf)(test time adaptation for OOD detection)
10. SIGKDD [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661)(æ¯ä¸ªåŸŸå­¦ä¹ å•ç‹¬çš„åˆ†ç±»å™¨ï¼Œæµ‹è¯•é˜¶æ®µæ ¹æ®entropyåŠ¨æ€ç»„åˆ)[[Code]](https://github.com/yfzhang114/AdaNPC)[[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
11. CVPR [Improved Test-Time Adaptation for Domain Generalization](https://arxiv.org/abs/2304.04494)(ä½¿ç”¨ä¸€ä¸ªå…·æœ‰å¯å­¦ä¹ å‚æ•°çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯é¢„å®šä¹‰çš„å‡½æ•°)


# Robutness/Adaptation/Fairness/OOD Detection
## 2022
1. Arxiv [Are Vision Transformers Robust to Spurious Correlations?](https://arxiv.org/pdf/2203.09125.pdf)(å¯¹ViTé²æ£’æ€§çš„ç ”ç©¶ï¼Œæ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒå‰æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¯¹ä¼ªç›¸å…³çš„é²æ£’æ€§ï¼Œé¢„è®­ç»ƒæ•°æ®è¾ƒå°‘åè€Œä¸å¦‚CNN)
2. CVPR [Exploring Domain-Invariant Parameters for Source FreeDomain Adaptation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Exploring_Domain-Invariant_Parameters_for_Source_Free_Domain_Adaptation_CVPR_2022_paper.pdf)(ç›¸æ¯”äºä»¥å¾€å·¥ä½œæ¢ç´¢åŸŸä¸å˜ç‰¹å¾ï¼Œè¯¥å·¥ä½œæ—¨åœ¨å¯»æ‰¾åŸŸä¸å˜å‚æ•°)
3. CVPR [CENet: Consolidation-and-Exploration Network for Continuous DomainAdaptation](https://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Zhang_CENet_Consolidation-and-Exploration_Network_for_Continuous_Domain_Adaptation_CVPRW_2022_paper.pdf)(æœ¬æ–‡è¯´ä»–æå‡ºäº†continuous DAçš„æ¦‚å¿µï¼Œä½†æ˜¯ICML 18å°±å·²ç»æå‡ºäº†å‘€ï¼Ÿ)
4. CVPR [Slimmable Domain Adaptation](https://arxiv.org/abs/2206.06620)(Adaptationçš„å¯¹è±¡ä¸ä»…åº”è¯¥æ˜¯æ•°æ®ï¼Œæœ¬æ–‡è€ƒè™‘ä¸‹æ¸¸è®¾å¤‡çš„adaptationã€‚)
5. NeurIPS Outstanding [Is Out-of-Distribution Detection Learnable?](https://arxiv.org/pdf/2210.14707.pdf)(å„ç§åœºæ™¯ä¸‹çš„OOD detectionçš„PACç†è®º)
6. ICML [Out-of-Distribution Detection with Deep Nearest Neighbors](https://arxiv.org/pdf/2204.06507.pdf)(ç”¨KNNåšOOD detection)
7. Arxiv [A Simple Test-Time Method for Out-of-Distribution Detection](https://arxiv.org/pdf/2207.08210.pdf)(test time adaptation for OOD detection)
8. Arxiv [RobArch: Designing Robust Architectures against Adversarial Attacks](https://shengyun-peng.github.io/papers/22_robarch.pdf)(å¯¹å¦‚ä½•è®¾è®¡é²æ£’æ€§æ›´å¼ºçš„æ¨¡å‹ç»“æ„åšäº†å¤§é‡çš„å®éªŒéªŒè¯)

## 2021
1. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
2. ICCV [Generalized Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/404697072)(ä¸ä½¿ç”¨æºåŸŸæ•°æ®ï¼Œåªæœ‰æºåŸŸé¢„è®­ç»ƒçš„æ¨¡å‹æ—¶å¦‚ä½•adaptationå¹¶ä¿è¯source domainçš„æ€§èƒ½)
3. ICCV [Adaptive Adversarial Network for Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(åœ¨æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èƒ½å¦å¯»æ‰¾ä¸€ç§æ–°çš„é’ˆå¯¹ç›®æ ‡çš„åˆ†ç±»å™¨ï¼Œå¹¶ä½¿å…¶é€‚åº”ç›®æ ‡ç‰¹å¾)
4. ICCV [Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(è¯¥ç®—æ³•é€šè¿‡ç‰¹å¾æå–å™¨å’Œé‰´åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—å­¦ä¹ æ¥å‡å°ç‰¹å¾æ¢¯åº¦åœ¨ä¸¤ä¸ªåŸŸä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚)
5. FAccT [Algorithmic recourse: from counterfactual explanations to interventions](https://zhuanlan.zhihu.com/p/424631782)(æå‡ºäº†causal recourseçš„æ¦‚å¿µ)
6. ICML WorkShop [On the Fairness of Causal Algorithmic Recourse](https://zhuanlan.zhihu.com/p/424631782)(æœ¬æ–‡åœ¨group recourseçš„åŸºç¡€ä¸Šè€ƒè™‘äº†å¤šä¸ªå˜é‡ä¹‹é—´çš„ç›¸äº’å½±å“å³æ‰€è°“çš„å› æœå…³ç³»ã€‚)
7. NeurIPS [Domain Adaptation with Invariant Representation Learning: What Transformations to Learn?](https://zhuanlan.zhihu.com/p/316265317)(DAä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªencoderï¼Ÿ)
8. NeurIPS [Gradual Domain Adaptation without Indexed Intermediate Domains](https://zhuanlan.zhihu.com/p/316265317)(æ²¡æœ‰domaparameterin labelçš„Gradual domain adaption(GDA))
9. NeurIPS [Implicit Semantic Response Alignment for Partial Domain Adaptation](https://zhuanlan.zhihu.com/p/316265317)(PDAå¦‚ä½•åˆ©ç”¨å¥½é¢å¤–ç±»)
10. NeurIPS [The balancing principle for parameter choice in distance-regularized domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(å¦‚ä½•æŒ‘é€‰åˆ†ç±»æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹çš„tradeoff parameter)
11. AAAI [Provable Guarantees for Understanding Out-of-distribution Detection](https://arxiv.org/pdf/2112.00787.pdf)(åŸºäºæ•°æ®æ˜¯é«˜æ–¯æ··åˆçš„å‡è®¾ç»™å‡ºæœ€ä¼˜densityä¼°è®¡æ–¹å¼)

## Before 2021
1. Available at Optimization Online [Kullback-Leibler Divergence Constrained Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(å¼€ç¯‡ä¹‹ä½œï¼Œä½¿ç”¨KLæ•£åº¦æ„é€ DROä¸­çš„uncertainty set)
2. ICLR 2018 Oral [Certifying Some Distributional Robustnesswith Principled Adversarial Training](https://zhuanlan.zhihu.com/p/381176721)(åŸºäº Wasserstein-ballæ„é€ uncertainty setï¼Œç”¨äºadversarial robustness)
3. ICML 2018 Oral [Does Distributionally Robust Supervised Learning Give Robust Classifiers?](https://zhuanlan.zhihu.com/p/381176721)(DROå°±ä¸€å®šæ¯”ERMå¥½ï¼Ÿä¸ä¸€å®šï¼å¿…é¡»å¼•å…¥é¢å¤–ä¿¡æ¯)
4. NeurIPS 2019 [Distributionally Robust Optimization and Generalization in Kernel Methods](https://zhuanlan.zhihu.com/p/381176721)(æœ¬æ–‡ä½¿ç”¨MMD(maximummean discrepancy)å¯¹uncertainty setè¿›è¡Œå»ºæ¨¡ï¼Œå¾—åˆ°äº†MMD DRO)
5. EMNLP 2019 [Distributionally Robust Language Modeling](https://zhuanlan.zhihu.com/p/381176721)(Coarse-grained mixture modelsåœ¨NLPä¸­çš„ç»å…¸æ¡ˆä¾‹)
6. Arxiv 2019 [Equalizing recourse across groups](https://zhuanlan.zhihu.com/p/424631782)(åŸºç¡€çš„recourseæµ‹é‡çš„æ˜¯å•ä¸ªæ ·æœ¬ï¼Œæœ¬æ–‡ç»™å‡ºäº†ä¸€ä¸ªgroupçº§åˆ«çš„recourseåº¦é‡ã€‚)
7. ICML 2020 Oral [Continuously indexed domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(è¿ç»­å˜åŒ–çš„domain)

# Causality

## Individual Treatment Estimation
1. ICML 2017 [Estimating individual treatment effect: generalization bounds and algorithms](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡ç¬¬ä¸€æ¬¡æå‡ºäº†ITEçš„æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨DAçš„ä¸€å¥—ç†è®ºå¯¹å…¶è¿›è¡Œboundï¼Œä¾æ¬¡è®¾è®¡äº†ä¸€å¥—è¡Œè€Œæœ‰æ•ˆçš„ç®—æ³•ã€‚)
2. NeurIPS 2019 [Adapting Neural Networks for the Estimation of Treatment Effects](https://zhuanlan.zhihu.com/p/426793887)(è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³æ˜¯è¿™æ ·çš„ï¼šæˆ‘ä»¬æ²¡å¿…è¦ä½¿ç”¨æ‰€æœ‰çš„åæ–¹å·®å˜é‡Xè¿›è¡Œadjustmentã€‚)
3. PNAS 2019 [Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶X-learnerï¼Œå½“å„ä¸ªtreatmentç»„çš„æ•°æ®éå¸¸ä¸å‡è¡¡çš„æ—¶å€™ï¼Œè¿™ç§æ¡†æ¶éå¸¸æœ‰æ•ˆã€‚)
4. AAAI 2020 [Learning Counterfactual Representations for Estimating Individual Dose-Response Curves](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†æ–°çš„metricï¼Œæ–°çš„æ•°æ®é›†ï¼Œå’Œè®­ç»ƒç­–ç•¥ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„treatmentçš„outcomeè¿›è¡Œä¼°è®¡ã€‚)
5. ICLR 2021 Oral: [VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡åŸºäºvarying coefficient modelï¼Œè®©æ¯ä¸ªtreatmentå¯¹åº”çš„branchæˆä¸ºtreatmentçš„å‡½æ•°ï¼Œè€Œä¸éœ€è¦å•ç‹¬è®¾è®¡branchï¼Œä¾æ¬¡è¾¾åˆ°çœŸæ­£çš„è¿ç»­æ€§ã€‚)
6. Arxiv 2021 [Neural Counterfactual Representation Learning for Combinations of Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡è€ƒè™‘æ›´å¤æ‚çš„æƒ…å†µï¼šå¤šç§treatmentå…±åŒä½œç”¨ã€‚)
7. NeurIPS 2021 Spotlight [On Inductive Biases for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2106.03765)(æœ¬æ–‡æå‡ºäº†æ–°æ¡†æ¶FlexTENetï¼Œç›´æ¥å¯¹æ¡ä»¶å› æœå€¼Ï„è¿›è¡Œä¼°è®¡ï¼Œè€Œä¸æ˜¯å¯¹Î¼1ï¼ŒÎ¼2åˆ†åˆ«ä¼°è®¡)
8. NeurIPS 2021 [Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms](https://arxiv.org/abs/2101.10943)(æœ¬æ–‡åˆ†æäº†è¿›æ¥è¿›è¡Œ individual treatment effectçš„å„ç§ç®—æ³•èŒƒå¼ï¼Œ)
9. Arxiv 2021 [Cycle-Balanced Representation Learning For Counterfactual Inference](å¯¹treatmentï¼Œcontrolä¸¤ä¸ªgroupåˆ†åˆ«encodeï¼Œç„¶åå¯¹æŠ—å­¦ä¹ å‡å°‘åŸŸå·®è·ï¼Œä¸ºäº†é˜²æ­¢åˆ†ç±»ä¿¡æ¯è¢«æŠ¹å»ï¼ŒåŠ ä¸Šcycle-consistanceçš„çº¦æŸï¼Œé‡æ„ç‰¹å¾ã€‚)


# Data-Centric/Prompt

## Data Centric
1. AISTATS 2019 [Towards Optimal Transport with Global Invariances](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å¯¹é½ä¸¤ä¸ªæ•°æ®é›†ï¼Ÿ)
2. NeurIPS 2020 [Geometric Dataset Distances via Optimal Transport](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å®šä¹‰ä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´çš„è·ç¦»ï¼Ÿ)
3. ICML 2021 [Dataset Dynamics via Gradient Flows in Probability Space](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•è¿›è¡Œæ•°æ®é›†ä¼˜åŒ–ï¼Œä½¿å¾—ä¸¤ä¸ªæ•°æ®é›†å°½å¯èƒ½çš„åƒï¼Ÿ)

## Prompts

1. ACL 2021 [WARP: Word-level Adversarial ReProgramming](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptå¼€ç¯‡ä¹‹ä½œ)
2. Arxiv 2021 **Stanford**[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptç”¨äºNLGçš„å„ç§ä»»åŠ¡)(å°†promptç”¨äºNLGä»»åŠ¡ä¸Š)
3. Arxiv 2021 **Google**[The Power of Scale for Parameter-Efficient Prompt Tuning](https://zhuanlan.zhihu.com/p/407144573)(ç›®å‰æœ€ç®€å•çš„preifx trainingï¼šåªå¯¹inputæ·»åŠ prefix)
4. Arxiv 2021 **DeepMind**[Multimodal Few-Shot Learning with Frozen Language Models](https://zhuanlan.zhihu.com/p/407144573)(åˆ©ç”¨å›¾åƒç¼–ç å™¨æŠŠå›¾åƒä½œä¸ºä¸€ç§åŠ¨æ€çš„prefixï¼Œä¸æ–‡æœ¬ä¸€èµ·é€å…¥LMä¸­)


# Optimization/GNN/Energy/Generative/Others

## Optimization
1. ICML 2021 [An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming](https://zhuanlan.zhihu.com/p/390808626)
2. NeurIPS 2021 _Deep Structural Causal Models for Tractable Counterfactual Inference_
1. ICML 2018 _Bilevel Programming for Hyperparameter Optimization and Meta-Learning_(ç”¨bi-level programmingå»ºæ¨¡è¶…å‚æ•°æœç´¢ä¸meta-learning)
2. NeurIPS 2021 [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)

## LTH (Lottery Ticket Hypothesis)
1. NeurIPS 2020: [The Lottery Ticket Hypothesis for Pre-trained BERT Networks](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºBERT fine-tune))
2. ICML 2021 Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºOODæ³›åŒ–)
3. CVPR 2021: [The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºè§†è§‰æ¨¡å‹é¢„è®­ç»ƒ)


## Generative Model (mainly diffusion model)
1. _Estimation of Non-Normalized Statistical Models by Score Matching_(ä½¿ç”¨åˆ†æ­¥ç§¯åˆ†ï¼ˆScore Matchingï¼‰çš„æ–¹æ³•è§£å†³éå½’ä¸€åŒ–åˆ†å¸ƒçš„ä¼°è®¡é—®é¢˜)
2. UAI 2019 _Sliced Score Matching: A Scalable Approach to Density and Score Estimation_(å°†é«˜ç»´çš„æ¢¯åº¦åœºæ²¿éšå³æ–¹å‘æŠ•å½±åˆ°ä¸€ç»´çš„æ ‡é‡åœºå†è¿›è¡Œscore-macthing) 
3. NeurIPS 2019 Oral _Generative Modeling by Estimating Gradients of the Data Distribution_(é€šè¿‡æ·»åŠ å™ªå£°çš„æ–¹æ³•ï¼Œå¢å¼ºLangevin MCMCå¯¹ä½æ¦‚ç‡å¯†åº¦åŒºåŸŸçš„å»ºæ¨¡èƒ½åŠ›)
4. NeurIPS 2020 _improved techniques for training score-based generative models_(å¯¹score-based generative modelå¤±è´¥æ¡ˆä¾‹çš„åˆ†æå’Œæ”¹è¿›ï¼Œç”Ÿæˆèƒ½åŠ›å¼€å§‹åª²ç¾GAN)
6. NeurIPS 2020 [Denoising Diffusion Probabilistic Models](https://zhuanlan.zhihu.com/p/366004028)(é™¤VAE, GAN, FLOWå¤–åˆä¸€ç”ŸæˆèŒƒå¼)
7. ICLR 2021 **Outstanding Paper Award** [Score-Based Generative Modeling through Stochastic Differential Equations](http://yang-song.github.io/blog/2021/score/)
8. Arxiv 2021 _Diffusion Models Beat GANs on Image Synthesis_(Diffusion Modelsåœ¨å›¾åƒå’Œåˆæˆä¸Šè¶…è¶ŠGAN) 
10. Arxiv 2021 Variational Diffusion Models

## Implicit Neural Representation (INR)
1. NeurIPS 2020 (Oral)ï¼š [Implicit Neural Representations with Periodic Activation Functions](https://zhuanlan.zhihu.com/p/472942119)
2. SIGGRAPH Asia 2020ï¼š [X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation](https://zhuanlan.zhihu.com/p/472942119)
3. CVPR 2021 (Oral)ï¼š[Learning Continuous Image Representation with Local Implicit Image Function](https://zhuanlan.zhihu.com/p/472942119)
4. CVPR 2021 [Adversarial Generation of Continuous Images](https://arxiv.org/abs/2011.12026)
5. NeurIPS 2021 [Learning Signal-Agnostic Manifolds of Neural Fields](https://arxiv.org/abs/2111.06387)
6. Arxiv 2021 [Generative Models as Distributions of Functions](https://arxiv.org/abs/2102.04776)


## Survey
1. [ç»¼è¿°ï¼šåŸºäºèƒ½é‡çš„æ¨¡å‹](https://zhuanlan.zhihu.com/p/343529491)
