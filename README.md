

This is a repository for organizing articles related to Domain generalization, OOD, optimization, data-centric learning, prompt learning, robutness, and causality. Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.

### About Me :high_brightness: 
I'm the second year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/), alibaba DAMO Academy, work with Prof. [Rong Jin](https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN).


###  ğŸ”¥ Updated 2023-10-30
- Recent Domain generalization, test time adaptation, and OOD detection papers on **ICCV 2023**, LLM safety have been updated.
- Our paper  [OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling](https://arxiv.org/abs/2309.12659) has been accepted by **NeurIPS 2023**.  [[Code]](https://github.com/yfzhang114/OneNet)[[Reading Notes]](https://zhuanlan.zhihu.com/p/658191974)
- Our paper  [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661) has been accepted by **SIGKDD 2023**. [[Code]](https://github.com/yfzhang114/AdaNPC) [[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
- Our paper  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566) has been accepted by **ICML 2023**. [[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
- Our paper [Free Lunch for Domain Adversarial Training: Environment Label Smoothing](https://arxiv.org/abs/2302.00194) has been accepted by **ICLR 2023**. [[Code]](https://github.com/yfzhang114/Environment-Label-Smoothing)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/600466715)
- Our paper [Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2202.01336) has been accepted by **NeurIPS ML Safety** workshop. [[Code]](https://github.com/hlzhang109/TransTEE)
- Our paper Towards Principled Disentanglement for Domain Generalization has been selected for an CVPR **ORAL** presentation. :blush: [[Reading Notes]](https://zhuanlan.zhihu.com/p/477855079) [[Code]](https://github.com/hlzhang109/DDG)  [[paper]](https://arxiv.org/abs/2111.13839)

# Table of Contents (ongoing)
* [Generalization/OOD](#generalizationood)
   * [2023](#2023)
   * [2022](#2022)
   * [2017-2021](#old-but-important)
* [LLM safety](#llm-safety)
* [Test-time adaptation](#test-time-adaptation)
* [Robutness/Adaptation/Fairness/OOD Detection](#robutnessadaptationfairnessood-detection)
   * [2022](#2022-1)
   * [Before 2022](#before-2022)
* [Data-Centric/Prompt/Large-Pretrain-Model](#data-centricpromptlarge-pretrain-model)
   * [Data Centric](#data-centric)
   * [Prompts](#prompts)
   * [Large-Pretrain-Model](#large-pretrain-model)
* [Optimization/GNN/Energy/Causality/Others](#optimizationgnnenergygenerativecausalityothers)
   * [Optimization](#optimization)
   * [Individual Treatment Estimation](#individual-treatment-estimation)
   * [LTH (Lottery Ticket Hypothesis)](#lth-lottery-ticket-hypothesis)
   * [Generative Model (Mainly Diffusion Model)](#generative-model-mainly-diffusion-model)
   * [Implicit Neural Representation (INR)](#implicit-neural-representation-inr)
   * [Survey](#survey)
# Generalization/OOD

## 2023
0. ICLR [Free Lunch for Domain Adversarial Training: Environment Label Smoothing](https://arxiv.org/abs/2302.00194)(ç¯å¢ƒæ ‡ç­¾å¹³æ»‘ï¼Œä¸€è¡Œä»£ç æå‡å¯¹æŠ—å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ³›åŒ–æ€§). [[Code]](https://github.com/yfzhang114/Environment-Label-Smoothing)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/600466715)
1. ICML  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566)(ç”¨KNNè¿›è¡Œæµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œä»ç†è®ºä¸Šåˆ†æäº†TTA workçš„åŸå› )[[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
2. ICLR [Out-of-Distribution Representation Learning for Time Series Classification](https://arxiv.org/abs/2209.07027)(ä»OODçš„è§’åº¦è€ƒè™‘æ—¶åºåˆ†ç±»çš„é—®é¢˜)
3. ICLR [Contrastive Learning for Unsupervised Domain Adaptation of Time Series](https://arxiv.org/abs/2206.06243)(ç”¨å¯¹æ¯”å­¦ä¹ å¯¹å…¶ç±»é—´åˆ†å¸ƒä¸ºæ—¶åºDAå­¦ä¸€ä¸ªå¥½çš„è¡¨å¾)
4. ICLR [Pareto Invarian Risk Minimization](https://openreview.net/forum?id=esFxSb_0pSL)(é€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–è§’åº¦ç†è§£ä¸ç¼“è§£OOD/DGä¼˜åŒ–éš¾é—®é¢˜)
5. ICLR [Fairness and Accuracy under Domain Generalization](https://arxiv.org/abs/2301.13323)(ä¸ä»…è€ƒè™‘æ³›åŒ–çš„æ€§èƒ½ï¼Œä¹Ÿè€ƒè™‘æ³›åŒ–çš„å…¬å¹³æ€§)
6. Arxiv [Adversarial Style Augmentation for Domain Generalization](https://arxiv.org/abs/2301.12643)(å¯¹æŠ—å­¦ä¹ æ·»åŠ å›¾åƒæ‰°åŠ¨ä»¥æå‡æ¨¡å‹æ³›åŒ–æ€§)
7. Arxiv [CLIPood: Generalizing CLIP to Out-of-Distributions](https://arxiv.org/abs/2302.00864)(ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œå…‹æœdomain shift and open classä¸¤ä¸ªé—®é¢˜)
9. SIGKDD [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661)(æ¯ä¸ªåŸŸå­¦ä¹ å•ç‹¬çš„åˆ†ç±»å™¨ï¼Œæµ‹è¯•é˜¶æ®µæ ¹æ®entropyåŠ¨æ€ç»„åˆ)[[Code]](https://github.com/yfzhang114/AdaNPC)[[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
10. CVPR [Federated Domain Generalization with Generalization Adjustment](https://scholar.google.com/scholar_url?url=https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf&hl=zh-CN&sa=X&d=13348506996942284912&ei=sTpvZIjhI9OQ6rQP29uDqAU&scisig=AGlGAw8T1YjQNN8nVv2lI6LPBiGS&oi=scholaralrt&hist=lUnt8X4AAAAJ:7797965790415635509:AGlGAw-zJ0qtstLHlwZtiYmf7uNN&html=&pos=1&folt=rel)(ä¸ºè”é‚¦åŸŸæ³›åŒ–(FedDG)æä¾›äº†ä¸€ä¸ªæ–°çš„æ–°çš„å‡å°æ–¹å·®çš„æ­£åˆ™é¡¹ä»¥é¼“åŠ±å…¬å¹³æ€§)
11. CVPR [Distribution Shift Inversion for Out-of-Distribution Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf)(TTAæ–¹æ³•ï¼Œå°†OoDæµ‹è¯•æ ·æœ¬ç”¨ä»…åœ¨æºåˆ†å¸ƒä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å‘è®­ç»ƒåˆ†å¸ƒè½¬ç§»ç„¶åå†æµ‹è¯•)
12. CVPR [SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization](https://arxiv.org/abs/2305.11615)(é€šè¿‡ç§»é™¤é‚£äº›å¼ºçƒˆä¾èµ–å·²è¯†åˆ«çš„è™šå‡ç‰¹å¾çš„ç½‘ç»œåˆ†æ”¯æ¥å®ç°modular risk minimization (MRM))
13. CVPR [Improved Test-Time Adaptation for Domain Generalization](https://arxiv.org/abs/2304.04494)(ä½¿ç”¨ä¸€ä¸ªå…·æœ‰å¯å­¦ä¹ å‚æ•°çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯é¢„å®šä¹‰çš„å‡½æ•°)
14. ICLR [Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization](https://openreview.net/forum?id=uyqks-LILZX)(çœŸå®ä¸–ç•Œçš„æ•°æ®é€šå¸¸åœ¨ä¸åŒå±æ€§ä¸Šæœ‰å¤šç§åˆ†å¸ƒåç§»ï¼Œç›®å‰DGç®—æ³•æ— æ³•workï¼Œæœ¬æ–‡åˆ©ç”¨æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„çŸ¥è¯†è‡ªé€‚åº”åœ°è¯†åˆ«å’Œåº”ç”¨æ­£ç¡®çš„æ­£åˆ™åŒ–çº¦æŸ)
15. ICLR [Using Language to Extend to Unseen Domains](https://openreview.net/forum?id=eR2dG8yjnQ)(åˆ©ç”¨CLIPæ¨¡å‹çš„çŸ¥è¯†å°†æºåŸŸå›¾åƒembeddingè½¬æ¢ä¸ºå¤šä¸ªç›®æ ‡åŸŸçš„representationï¼ˆä»photos of birdsè½¬åŒ–ä¸ºpaintings of birdsï¼‰)
16. ICLR [How robust is unsupervised representation learning to distribution shift?](https://openreview.net/forum?id=LiXDW7CF94J)(æ— ç›‘ç£å­¦ä¹ ç®—æ³•ä¸­å­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨å„ç§æç«¯å’Œç°å®åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ³›åŒ–æ•ˆæœä¼˜äºSL)
17. ICLR [PGrad: Learning Principal Gradients For Domain Generalization](https://openreview.net/forum?id=CgCmwcfgEdH)(æµ‹é‡äº†æ‰€æœ‰è®­ç»ƒåŸŸçš„è®­ç»ƒåŠ¨æ€,æœ€ç»ˆçš„æ¢¯åº¦èšåˆäº†å¹¶ç»™å‡ºä¸€ä¸ªé²æ£’çš„ä¼˜åŒ–æ–¹å‘ï¼Œæœ‰ç‚¹åƒmeta-learning)
18. ICLR [Causal Balancing for Domain Generalization](https://openreview.net/forum?id=F91SROvVJ_6)(æå‡ºäº†ä¸€ç§å¹³è¡¡çš„å°æ‰¹é‡æŠ½æ ·ç­–ç•¥ï¼Œå°†æœ‰åå·®çš„æ•°æ®åˆ†å¸ƒè½¬æ¢ä¸ºå¹³è¡¡åˆ†å¸ƒï¼ŒåŸºäºæ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„æ½œåœ¨å› æœæœºåˆ¶çš„ä¸å˜æ€§ã€‚)
19. ICLR [Cycle-consistent Masked AutoEncoder for Unsupervised Domain Generalization](https://openreview.net/forum?id=wC98X1qpDBA)(æ— ç›‘ç£åŸŸæ³›åŒ–(UDG)ï¼Œå…¶ä¸­ä¸éœ€è¦æˆå¯¹çš„æ•°æ®æ¥è¿æ¥ä¸åŒçš„åŸŸã€‚è¿™ä¸ªé—®é¢˜çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œä½†åœ¨DGèƒŒæ™¯ä¸‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚)
20. Arxiv [Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations](https://arxiv.org/pdf/2306.04618.pdf)(æ³›åŒ–ç®—æ³•åœ¨NLP benchmarkä¸Šçš„è¡¨ç°ä¸æ¯”fully finetuneå¥½å¤šå°‘ç‰¹åˆ«æ˜¯IDæ•°æ®è¶³å¤Ÿå¤šæ—¶)
21. Arixv [A Survey on Out-of-Distribution Evaluation of Neural NLP Models](https://arxiv.org/pdf/2306.15261.pdf)(ç³»ç»Ÿè¯„ä¼°Adversarial robustness, domain generalization and dataset biases)

## 2022

0. CVPR Oral [Towards Principled Disentanglement for Domain Generalization](https://zhuanlan.zhihu.com/p/477855079)(å°†è§£è€¦ç”¨äºDGï¼Œæ–°ç†è®ºï¼Œæ–°æ–¹æ³•)
1. Arxiv [How robust are pre-trained models to distribution shift?](https://arxiv.org/abs/2206.08871)(è‡ªç›‘ç£æ¨¡å‹æ¯”æœ‰ç›‘ç£ä»¥åŠæ— ç›‘ç£æ¨¡å‹æ›´é²æ£’ï¼Œåœ¨å°éƒ¨åˆ†OODæ•°æ®ä¸Šé‡æ–°è®­ç»ƒclassifieræå‡å¾ˆå¤§)
2. ICML [A Closer Look at Smoothness in Domain Adversarial Training](https://arxiv.org/abs/2206.08213)(å¹³æ»‘åˆ†ç±»æŸå¤±å¯ä»¥æé«˜åŸŸå¯¹æŠ—è®­ç»ƒçš„æ³›åŒ–æ€§èƒ½)
3. CVPR [Bayesian Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/528829486)(ç¼“è§£IRMåœ¨æ¨¡å‹è¿‡æ‹Ÿåˆæ—¶é€€åŒ–ä¸ºERMçš„é—®é¢˜)
4. CVPR [Towards Unsupervised Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(å…³æ³¨æ¨¡å‹é¢„è®­ç»ƒçš„è¿‡ç¨‹å¯¹DGä»»åŠ¡çš„å½±å“ï¼Œè®¾è®¡äº†ä¸€ä¸ªåœ¨DGæ•°æ®é›†æ— ç›‘ç£é¢„è®­ç»ƒçš„ç®—æ³•)
5. CVPR [PCL: Proxy-based Contrastive Learning for Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(ç›´æ¥é‡‡ç”¨æœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ç”¨äºDGæ•ˆæœå¹¶ä¸å¥½ï¼Œæœ¬æ–‡æå‡ºå¯è¡Œæ–¹æ³•)
6. CVPR [Style Neophile: Constantly Seeking Novel Styles for Domain Generalization](https://zhuanlan.zhihu.com/p/528829486)(æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å¤šé£æ ¼çš„æ•°æ®)
7. Arxiv [WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks](https://woods-benchmarks.github.io/)(ä¸€ä¸ªå…³äºæ—¶åºæ•°æ®OODçš„å¤šä¸ªbenchmark)
8. Arxiv [A Broad Study of Pre-training for Domain Generalization and Adaptation](https://arxiv.org/pdf/2203.11819.pdf)(æ·±å…¥ç ”ç©¶äº†é¢„è®­ç»ƒå¯¹äºDA,DGä»»åŠ¡çš„ä½œç”¨ï¼Œç®€å•çš„ä½¿ç”¨ç›®å‰æœ€å¥½çš„backboneè¶³å·²å–å¾—SOTAçš„æ•ˆæœ)
9. Arxiv [Domain Generalization by Mutual-Information Regularization with Pre-trained Models](https://arxiv.org/pdf/2203.10789.pdf)(ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æŒ‡å¯¼finetuneçš„è¿‡ç¨‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›)
10. ICLR Oral [A Fine-Grained Analysis on Distribution Shift](https://zhuanlan.zhihu.com/p/466675818)(å¦‚ä½•å‡†ç¡®çš„å®šä¹‰distribution shiftï¼Œä»¥åŠå¦‚ä½•ç³»ç»Ÿçš„æµ‹é‡æ¨¡å‹çš„é²æ£’æ€§)
11. ICLR Oral [Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution](https://zhuanlan.zhihu.com/p/466675818)(fine-tuningï¼ˆå¾®è°ƒï¼‰å’Œlinear probingç›¸è¾…ç›¸æˆ)
12. ICLR Spotlight [Towards a Unified View of Parameter-Efficient Transfer Learning](https://zhuanlan.zhihu.com/p/466675818)(ç»Ÿä¸€çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç†è®ºæ¡†æ¶)
13. ICLR Spotlight [How Do Vision Transformers Work?](https://zhuanlan.zhihu.com/p/466675818)(Vision Transformers (ViTs)çš„ä¼˜è‰¯ç‰¹æ€§)
14. ICLR Spotlight [On Predicting Generalization using GANs](https://zhuanlan.zhihu.com/p/466675818)(ä½¿ç”¨æºåŸŸæ•°æ®è®­ç»ƒå‡ºçš„GANæ¥é¢„æµ‹æµ‹è¯•è¯¯å·®)
15. ICLR Poster [Uncertainty Modeling for Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/466675818)(åŸŸæ³›åŒ–æ—¶è€ƒè™‘ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•)
16. ICLR Poster [Gradient Matching for Domain Generalization](https://zhuanlan.zhihu.com/p/466675818)(é¼“åŠ±æ¥è‡ªä¸åŒåŸŸçš„æ¢¯åº¦ä¹‹é—´çš„å†…ç§¯æ›´å¤§)
17. ICML [DNA: Domain Generalization with Diversified Neural Averaging](https://zhuanlan.zhihu.com/p/553511043)(classifier ensembleï¼Œå³å¯¹åˆ†ç±»å™¨è¿›è¡Œé›†æˆã€‚æœ¬æ–‡ä»ç†è®ºå’Œå®éªŒè§’åº¦è®¨è®ºäº†ensembleä¸DGä»»åŠ¡çš„connectionã€‚)
18. ICML [Model Agnostic Sample Reweighting for Out-of-Distribution Learning](https://zhuanlan.zhihu.com/p/553511043)(bi-levelçš„å»æ‰¾ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬åŠ æƒæ–¹å¼)
19. ICML [Sparse Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/553511043)(åˆ©ç”¨å…¨å±€ç¨€ç–æ€§çº¦æ¥é˜²æ­¢ä¼ªç‰¹å¾åœ¨è®­ç»ƒè¿‡ç¨‹è¢«ä½¿ç”¨)
20. Arxiv [grounding visual representations with texts for domain generalization](http://arxiv.org/abs/2207.10285)(ç”¨è·¨æ¨¡æ€çš„æ•°æ®ä½œä¸ºæ¨¡å‹çš„ç›‘ç£ä¿¡æ¯å¯ä»¥æå‡æ³›åŒ–æ€§)
21. Arxiv [On the Strong Correlation Between Model Invarianceand Generalization](http://arxiv.org/abs/2207.07065)(æ¨¡å‹é¢„æµ‹çš„ä¸å˜æ€§ä¸æ³›åŒ–æ€§æœ‰å¼ºç›¸å…³ï¼Œè¿™é‡Œçš„ä¸å˜æ€§æ˜¯å¯¹xä¸åŒperturbationé¢„æµ‹çš„ä¸å˜æ€§)
22. NeurIPS [Probable Domain Generalization via Quantile Risk Minimization](https://arxiv.org/abs/2207.09944)(å°†DGå»ºæ¨¡æˆæ¦‚ç‡æ³›åŒ–çš„é—®é¢˜ï¼Œæ—¢ä¸æ˜¯worst-caseï¼Œä¹Ÿä¸æ˜¯average performance)
23. NeurIPS [Improving Multi-Task Generalization via Regularizing Spurious Correlation](https://arxiv.org/abs/2205.09797)(å»é™¤å¯¹ä»»åŠ¡æ ‡ç­¾çš„è™šå‡ä¾èµ–ï¼Œä»è€Œæå‡å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆæœ)
24. NeurIPS [Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction](https://arxiv.org/abs/2206.07085)(ä»ç†è®ºä¸Šè§£é‡Šå½’ä¸€åŒ–å±‚ä½¿å¾—æŸå¤±é¢é”åº¦é™ä½ï¼ŒGDæ›´æ˜“ä¼˜åŒ–)
25. NeurIPS [Assaying Out-Of-Distribution Generalization in Transfer Learning](https://zhuanlan.zhihu.com/p/573246040)(å…¨é¢çš„å¯¹æ¨¡å‹é²æ£’æ€§çš„å®šä¹‰æå‡ºæ–°çš„è§è§£)
26. NeurIPS [On the Strong Correlation Between Model Invariance and Generalization](https://zhuanlan.zhihu.com/p/573246040)(å¯¹å¯¹æ³›åŒ–ä¸ä¸å˜æ€§ä¹‹é—´çš„å…³ç³»è¿›è¡Œå®šé‡çš„åˆ†æ)
27. NeurIPS [Ensemble of Averages: Improving Model Selectionand Boosting Performance in Domain Generalization](https://zhuanlan.zhihu.com/p/573246040)(è®­ç»ƒè¿‡ç¨‹ä¸­OODæ•°æ®æ€§èƒ½æ³¢åŠ¨å¾ˆå¤§)
28. NeurIPS [Diverse Weight Averaging for Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/573246040)(æ²¿ç€è®­ç»ƒè½¨è¿¹å¹³å‡è·å¾—çš„æƒé‡)
29. NeurIPS [Improving Out-of-Distribution Generalization byAdversarial Training with Structured Priors](https://openreview.net/forum?id=Ku1afTnmozi)(ä½¿ç”¨domain specific structured low-rank perturbationsæ¥å¯¹æŠ—å­¦ä¹ æå‡OODæ€§èƒ½)
29. NeurIPS Outstanding [On-Demand Sampling:Learning Optimally from Multiple Distributions](https://arxiv.org/pdf/2210.12529.pdf)(ä¸€ä¸ªæœ‰ç†è®ºä¿è¯çš„å¤šåŸŸå­¦ä¹ ç®—æ³•ï¼Œè¾¾åˆ°äº†ç›®å‰æœ€ä½çš„sample complexity)
30. Arxiv [On Feature Learning in the Presence of Spurious Correlations](http://arxiv.org/abs/2210.11369)(ERMå·²ç»èƒ½å¤Ÿå­¦åˆ°å¾ˆå¥½çš„ç‰¹å¾äº†)
31. Arxiv [Simulating Bandit Learning from User Feedback for Extractive Question Answering](https://arxiv.org/abs/2203.10079)(å¼•å…¥å°‘é‡human evaluationå¯ä»¥æå‡æ¨¡å‹æ³›åŒ–æ€§)
32. ICLR [Uncertainty Modeling for Out-of-Distribution Generalization](https://arxiv.org/abs/2202.03958)(æ”¹å˜å›¾è±¡å‡å€¼/æ–¹å·®æ¥åšæ•°æ®å¢å¼ºï¼Œå‡å€¼æ–¹å·®è€ƒè™‘batchä¸­çš„ä¸ç¡®å®šæ€§)

## OLD but Important
****
**2021**

1. ICML [Improved OOD Generalization via Adversarial Training and Pre-training](https://proceedings.mlr.press/v139/yi21a.html)(ä»ç†è®ºä¸Šè¡¨æ˜ï¼Œä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œé‚£ä¹ˆå¯¹ä¸‹æ¸¸OODæ•°æ®çš„æ³›åŒ–å¯ä»¥æä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚)
2. ICCV [CrossNorm and SelfNorm for Generalization under Distribution Shifts](https://zhuanlan.zhihu.com/p/426728622)(æ€è·¯ç®€å•çš„æ­£åˆ™åŒ–æŠ€æœ¯ç”¨äºDG)
3. ICCV [A Style and Semantic Memory Mechanism for Domain Generalization](https://zhuanlan.zhihu.com/p/426728622)(å°è¯•ç€å»ä½¿ç”¨intra-domain style invarianceæ¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½)
4. Arxiv: [Towards a Theoretical Framework of Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆæ–°ç†è®ºï¼‰
5. Arxiv(**Yoshua Bengio**) _Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization_ (å½“OODé‡åˆ°ä¿¡æ¯ç“¶é¢ˆç†è®º)
6. Arxiv _Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation_
7. Arxiv _Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation_(ä½¿ç”¨çŸ¥è¯†è’¸é¦ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µ)
8. Arxiv _Delving Deep into the Generalization of Vision Transformers under Distribution Shifts_ (è§†è§‰transformerçš„æ³›åŒ–æ€§è®¨è®º)
9. Arxiv _Training Data Subset Selection for Regression with Controlled Generalization Error_ (ä»å¤§é‡è®­ç»ƒå®ä¾‹ä¸­é€‰æ‹©æ•°æ®å­é›†ï¼Œå¹¶ä¿æŒå¯æ¯”çš„æ³›åŒ–æ€§)
10. Arxiv(**MIT**) _Measuring Generalization with Optimal Transport_ (ç½‘ç»œå¤æ‚åº¦ä¸æ³›åŒ–æ€§çš„ç†è®ºç ”ç©¶ï¼Œ)
11. Arxiv(**SJTU**) [OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms](https://view.inews.qq.com/a/20210615A04V1C00?tbkt=B1&uid=) (æ­ç¤ºOODçš„è¯„æµ‹æ ‡å‡†å°šä¸å®Œå–„å¹¶æå‡ºè¯„æµ‹æ–¹æ¡ˆ)
12. Arxiv (Tsinghu) _Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization_ (æ–°çš„taskï¼šæ— ç›‘ç£çš„DGï¼ŒæºåŸŸçš„æ•°æ®æ ‡ç­¾ä¸å¯ä»¥ç”¨)
13. ICML Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆå½©ç¥¨æ¨¡å‹å¯»æ‰¾æ¨¡å‹ä¸­æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„å°æ¨¡å‹ï¼‰
14. ICML Oralï¼š[Domain Generalization using Causal Matching](https://zhuanlan.zhihu.com/p/382608823) (contrastive lossç‰¹å¾å¯¹é½+ç‰¹å¾ä¸å˜æ€§çº¦æŸ)
15. ICML Oral: _Just Train Twice: Improving Group Robustness without Training Group Information_
16. ICML Spotlight: [Environment Inference for Invariant Learning](https://zhuanlan.zhihu.com/p/382608823) (æ²¡æœ‰åŸŸæ ‡ç­¾å¦‚ä½•å­¦ä¹ åŸŸä¸å˜æ€§ç‰¹å¾ï¼Ÿ)
17. ICLR Poster: [Understanding the failure modes of out-of-distribution generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆOODå¤±è´¥çš„ä¸¤ç§åŸå› ï¼‰
18. ICLR Poster: [An Empirical Study of Invariant Risk Minimization](https://openreview.net/forum?id=jrA5GAccy_)(å¯¹IRMçš„å®éªŒæ€§æ¢ç´¢ï¼Œå¦‚å¯è§åŸŸçš„diversityå¦‚ä½•å½±å“IRMæ€§èƒ½ç­‰)
19. ICLR Poster _In Search of Lost Domain Generalization_ (æ²¡æœ‰model selectionçš„æ–¹æ³•ä¸æ˜¯å¥½æ–¹æ³•ï¼Œå¦‚ä½•æ ¹æ®éªŒè¯é›†é€‰æ‹©æ¨¡å‹ï¼Ÿ)
20. ICLR Poster [Modeling the Second Player in Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(ç”¨å¯¹æŠ—å­¦ä¹ å»ºæ¨¡DROçš„uncertainty set)
21. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
22. ICLR Spotlight(**Yoshua Bengio**) [Systematic generalisation with group invariant predictions](https://zhuanlan.zhihu.com/p/382608823) (å°†æ¯ä¸ªç±»åˆ†æˆä¸åŒçš„domain(_environment inference_ï¼Œç„¶åçº¦æŸæ¯ä¸ªåŸŸçš„ç‰¹å¾å°½å¯èƒ½ä¸€è‡´ä»è€Œé¿å…è™šå‡ä¾èµ–))
23. CVPR Oral: [Reducing Domain Gap by Reducing Style Bias](https://zhuanlan.zhihu.com/p/382608823) (channel-wise å‡å€¼ä½œä¸ºå›¾åƒé£æ ¼ï¼Œå‡å°‘CNNå¯¹é£æ ¼çš„ä¾èµ–)
24. AISTATS _Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions_
25. AISTATS Oral _Does Invariant Risk Minimization Capture Invariance_(IRMåªæœ‰åœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„æƒ…å†µä¸‹æ‰èƒ½çœŸæ­£æ•æ‰ä¸å˜å½¢ç‰¹å¾)
26. NeurIPS [Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests](https://arxiv.org/abs/2106.00545)(æœ¬æ–‡ä½¿ç”¨å› æœå·¥å…·è®¾è®¡äº†ä¸€ä¸ªå¯è¡Œçš„ç®—æ³•ï¼Œå°†åäº‹å®æ¨ç†ä¸åŸŸæ³›åŒ–ï¼ˆOODï¼‰è”ç³»èµ·æ¥ï¼Œè¿›è¡Œæœ‰æ•ˆçš„â€œstress testâ€ï¼Œæ¯”å¦‚å˜åŒ–ä¸€ä¸ªå¥å­åŒ…å«çš„çš„genderä¿¡æ¯ï¼Œçœ‹æœ€åæƒ…æ„Ÿåˆ†ç±»ä¼šä¸ä¼šæ”¹å˜ã€‚)
27. NeurIPS [Adaptive Risk Minimization: Learning to Adapt to Domain Shift](https://zhuanlan.zhihu.com/p/357962431)(åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥æ›´å¥½åœ°å¤„ç†æ–°domainå¼•èµ·çš„distribution shift)
28. NeurIPS [An Empirical Investigation of Domain Generalization with Empirical Risk Minimizers](https://zhuanlan.zhihu.com/p/357962431)(åŸºäºdomain adaptationçš„ç†è®ºæµ‹é‡æ–¹æ³•ä¸èƒ½å‡†ç¡®åœ°æ•æ‰OODæ³›åŒ–è¡Œä¸º)
29. NeurIPS Spotlight [Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization](https://zhuanlan.zhihu.com/p/357962431)(åœ¨testçš„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°æ¨¡å‹å¤´éƒ¨çš„linearå±‚)
30. NeurIPS [Why Do Better Loss Functions Lead to Less Transferable Features?](https://zhuanlan.zhihu.com/p/357962431)(æœ¬æ–‡ç ”ç©¶äº†è®­ç»ƒç›®æ ‡çš„é€‰æ‹©å¦‚ä½•å½±å“å·ç§¯ç¥ç»ç½‘ç»œåœ¨ImageNetä¸Šè®­ç»ƒå¾—åˆ°çš„å¯è¿ç§»æ€§)ã€
****

1. Arxiv 2020 [I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models](https://zhuanlan.zhihu.com/p/288980706)(å°†Domain Adaptationçœ‹ä½œæ˜¯å› æœå›¾æ¨ç†é—®é¢˜)
2. Arxiv 2020 (**Stanford**)_Distributionally Robust Lossesfor Latent Covariate Mixtures_.
3. NeurIPS 2020 [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)(ä½¿ç”¨èƒ½é‡æ¨¡å‹æ£€æµ‹OODæ ·æœ¬)
4. NeurIPS 2020 _Fairness without demographics through adversarially reweighted learning_ (åˆ©ç”¨å¯¹æŠ—å­¦ä¹ å¯¹éš¾æ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œå¸Œæœ›åŠ æƒåçš„æ ·æœ¬ä½¿å¾—åˆ†ç±»å™¨çš„æŸå¤±æ›´å¤§)
5. NeurIPS 2020 _Self-training Avoids Using Spurious Features Under Domain Shift_ (ä½¿ç”¨target domainçš„æ— æ ‡ç­¾æ•°æ®è®­ç»ƒæœ‰åŠ©äºé¿å…ä½¿ç”¨è™šå‡ç‰¹å¾)
6. NeurIPS 2020 _What shapes feature representations? Exploring datasets, architectures, and training_(Simplicity Biasï¼Œç¥ç»ç½‘ç»œå€¾å‘äºæ‹Ÿåˆâ€œå®¹æ˜“â€çš„ç‰¹å¾)
7. Arxiv 2020 [Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (å¥ åŸºä¹‹ä½œï¼Œè·³å‡ºç»éªŒé£é™©æœ€å°åŒ–--ä¸å˜é£é™©æœ€å°åŒ–)
8. ICLR 2020 Poster [The Risks of Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (ä¸å˜é£é™©æœ€å°åŒ–çš„ç¼ºé™·:åŸŸæ•°ç›®è¿‡å°‘IRMå³å¤±è´¥)
9. ICLR 2020 _Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization_(GroupDRO: æ‹¥æœ‰å¼ºæ­£åˆ™çš„DRO)
10. ICML 2020 _An investigation of why overparameterizationexacerbates spurious correlations_(ç¥ç»ç½‘ç»œçš„è¿‡å‚æ•°åŒ–æ˜¯é€ æˆç½‘ç»œä½¿ç”¨è™šå‡ç›¸å…³æ€§çš„é‡è¦åŸå› )
11. ICML 2020 UDA workshop _Learning Robust Representations with Score Invariant Learning_(éå½’ä¸€åŒ–ç»Ÿè®¡æ¨¡å‹ï¼šç”¨èƒ½é‡å­¦ä¹ çš„æ–¹å¼åšOOD)
12. ICML 2018 Oral (**Stanford**) _Fairness Without Demographics in Repeated Loss Minimization._
13. ICCV 2017 [CCSA--Unified Deep Supervised Domain Adaptation and Generalization](https://blog.csdn.net/Adupanfei/article/details/85165667) (å¯¹æ¯”æŸå¤±å¯¹é½æºåŸŸç›®æ ‡åŸŸæ ·æœ¬ç©ºé—´)
14. JSTOR (**Peters**)Causal inference by using invariant prediction: identification and confidence intervals.
15. ICML 2015 [Towards a Learning Theory of Cause-Effect Inference](ä½¿ç”¨kernel mean embeddingå’Œåˆ†ç±»å™¨è¿›è¡Œcasual inference                  )
16. IJCAI 2020 (**CMU**) _Causal Discovery from Heterogeneous/Nonstationary Data_

****
**Survey**

1. [Causality åŸºç¡€æ¦‚å¿µæ±‡æ€»](https://zhuanlan.zhihu.com/p/269625734)
2. [Domain AdaptationåŸºç¡€æ¦‚å¿µä¸ç›¸å…³æ–‡ç« è§£è¯»](https://zhuanlan.zhihu.com/p/272508224)
****

# LLM Safety

1. [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://zhuanlan.zhihu.com/p/664096097)(ä½œè€…æè¿°äº†ä»–ä»¬æ—©æœŸè¿›è¡Œæ‰‹åŠ¨çº¢é˜Ÿæµ‹è¯•çš„åŠªåŠ›ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å®‰å…¨æ€§å¹¶æµ‹é‡æ¨¡å‹çš„å®‰å…¨æ€§)
2. [Jailbroken: How Does LLM Safety Training Fail?](https://zhuanlan.zhihu.com/p/664096097)(è°ƒæŸ¥ä¸ºä»€ä¹ˆè¿™äº›è¶Šç‹±æ”»å‡»æˆåŠŸä»¥åŠå®ƒä»¬å¦‚ä½•ç”Ÿæˆçš„ã€‚ç«äº‰ç›®æ ‡å’Œä¸åŒ¹é…çš„æ³›åŒ–)
3. [Constitutional AI: Harmlessness from AI Feedback](https://zhuanlan.zhihu.com/p/664096097)(é€šè¿‡AIæŒ‡å¯¼æ¥ç”Ÿå¼€å‘ä¸€ä¸ªæœ‰å¸®åŠ©ã€è¯šå®ã€æ— å®³ä¸”ä¸ä¼šè§„é¿é—®é¢˜çš„AIåŠ©æ‰‹)
4. [Generative Judge for Evaluating Alignment](https://zhuanlan.zhihu.com/p/664096097)(AUTO-Jï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„è¯„ä¼°scoreï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°LLMsåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚)
5. [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://zhuanlan.zhihu.com/p/664096097)(é€šè¿‡æ“çºµdecodingæ–¹æ³•çš„å˜åŒ–æ¥ç ´åæ¨¡å‹çš„å¯¹é½ã€‚)
6. [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://zhuanlan.zhihu.com/p/664096097)(AutoDANå¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åˆ†å±‚é—ä¼ ç®—æ³•è‡ªåŠ¨ç”Ÿæˆéšè”½è¶Šç‹±æç¤ºã€‚)
7. [Are aligned neural networks adversarially aligned?](https://zhuanlan.zhihu.com/p/664096097)( å¯¹å¤šæ¨¡æ€æ¨¡å‹ï¼ŒéNLPçš„æ–¹æ³•è¿›è¡Œè¶Šç‹±æ•ˆæœæ˜¯å¾ˆä¸é”™çš„,è¿™ç§é”™ä½æ”»å‡»æ˜¯ååˆ†å±é™©çš„, å¹¶ä¸”ç›®å‰çš„å¯¹é½æŠ€æœ¯æ— æ³•é˜²èŒƒæ­¤ç±»æ”»å‡».)
8. [On the Exploitability of Instruction Tuning](https://zhuanlan.zhihu.com/p/664096097)(è¿™é¡¹ç ”ç©¶æ—¨åœ¨æ¢ç©¶å¦‚ä½•é€šè¿‡å‘è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥ç‰¹å®šçš„éµå¾ªæŒ‡ä»¤ç¤ºä¾‹ï¼Œä»è€Œæœ‰æ„æ”¹å˜æ¨¡å‹è¡Œä¸ºçš„æ–¹å¼)

# Test-time Adaptation

0. ICML  [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/abs/2304.12566)(ç”¨KNNè¿›è¡Œæµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œä»ç†è®ºä¸Šåˆ†æäº†TTA workçš„åŸå› )[[Code]](https://github.com/yfzhang114/AdaNPC)  [[Reading Notes]](https://zhuanlan.zhihu.com/p/624770864)
1. NeurIPS 2021 [Spotlight] [Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization](https://zhuanlan.zhihu.com/p/559916666)(åœ¨testçš„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°æ¨¡å‹å¤´éƒ¨çš„linearå±‚)
2. CVPR 2021 [Adaptive Methods for Real-World Domain Generalization](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶è¾“å…¥source domain embeddingï¼Œå³testæ—¶åˆ©ç”¨domainä¿¡æ¯)
3. ICLR 2021 [Spotlight] [Tent: Fully Test-Time Adaptation by Entropy Minimization](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶æœ€å°åŒ–æ¨¡å‹é¢„æµ‹çš„entropy)
4. ICCV 2021 [Test-Agnostic Long-Tailed Recognition y Test-Time Aggregating Diverse Experts with Self-Supervision](https://zhuanlan.zhihu.com/p/559916666)(æµ‹è¯•æ—¶ä¼˜åŒ–æ ·æœ¬çš„è‡ªç›‘ç£æŸå¤±)
5. NeurIPS 2022 [Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering](https://arxiv.org/abs/2206.02721)(å‘ç°æºå’Œç›®æ ‡åŸŸä¸­çš„é›†ç¾¤ï¼Œå¹¶å°†ç›®æ ‡é›†ç¾¤ä¸æºé›†ç¾¤è¿›è¡ŒåŒ¹é…ï¼Œä»¥æ”¹è¿›æ³›åŒ–ã€‚)
6. NeurIPS 2022 [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2209.07511)(æµ‹è¯•é˜¶æ®µæ ¹æ®æœ€å°åŒ–é¢„æµ‹ç†µä»è€Œæ›´æ–°prompt)
7. NeurIPS 2022 [MEMO: Test Time Robustness via Adaptation and Augmentation](https://arxiv.org/abs/2110.09506)(æµ‹è¯•é˜¶æ®µæ•°æ®å¢å¼º+æœ€å°åŒ–ç†µ)
8. NeurIPS 2022 [Test-Time Adaptation via Conjugate Pseudo-labels](https://arxiv.org/abs/2207.09640)(å¯¹äºç°å­˜çš„æœ‰/æ— ç›‘ç£çš„TTA lossï¼Œå¯ä»¥é€šè¿‡å¯¹å¶çš„æ¨å¯¼ç®€å•çš„å¾—åˆ°æœ€ä¼˜TTA lossï¼ˆä»å¹¿æ³›çš„å‡½æ•°ç±»åˆ«ä¸­è¿›è¡Œå…ƒå­¦ä¹ ä»¥è·å¾—çš„æœ€ä½³çš„TTAæŸå¤±ï¼‰ï¼Œè¿™ä¸ªlosså¯ä»¥è½¬åŒ–ä¸ºä¸€ç±»ç‰¹æ®Šçš„ä¼ªæ ‡ç­¾ï¼Œè¢«ç§°ä¸º Conjugate Pseudo-labels)
9. CVPR 2022 [Continual Test-Time Domain Adaptation](https://arxiv.org/abs/2203.13591)(ä»ä¸€ä¸ªæºåŸŸadaptåˆ°ä¸€ç³»åˆ—è¿ç»­æ”¹å˜çš„ç›®æ ‡åŸŸ)
10. Arxiv [A Simple Test-Time Method for Out-of-Distribution Detection](https://arxiv.org/pdf/2207.08210.pdf)(test time adaptation for OOD detection)
11. SIGKDD 2023 [Domain-Specific Risk Minimization for Out-of-Distribution Generalization](https://arxiv.org/abs/2208.08661)(æ¯ä¸ªåŸŸå­¦ä¹ å•ç‹¬çš„åˆ†ç±»å™¨ï¼Œæµ‹è¯•é˜¶æ®µæ ¹æ®entropyåŠ¨æ€ç»„åˆ)[[Code]](https://github.com/yfzhang114/AdaNPC)[[Reading Notes]](https://zhuanlan.zhihu.com/p/631524930)
12. CVPR 2023 [Improved Test-Time Adaptation for Domain Generalization](https://arxiv.org/abs/2304.04494)(ä½¿ç”¨ä¸€ä¸ªå…·æœ‰å¯å­¦ä¹ å‚æ•°çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯é¢„å®šä¹‰çš„å‡½æ•°)
13. CVPR 2023 [Feature Alignment and Uniformity for Test Time Adaptation](https://arxiv.org/abs/2303.10902)(å°†TTAä½œä¸ºä¸€ä¸ªç”±äºæºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åŸŸå·®è·è€Œå¯¼è‡´çš„ç‰¹å¾ä¿®è®¢é—®é¢˜:ç¡®ä¿å½“å‰æ‰¹å’Œæ‰€æœ‰å…ˆå‰æ‰¹ä¹‹é—´çš„è¡¨ç¤ºä¹‹é—´çš„å‡åŒ€æ€§,ä¸€è‡´æ€§)
14. CVPR 2023 [TIPI: Test Time Adaptation with Transformation Invariance](https://atuannguyen.com/assets/pdf/nguyen2023tipi.pdf)(ä¸ºäº†å…‹æœå°batchçš„é—®é¢˜æå‡ºäº†ä¸€ä¸ªæ–°çš„loss)
15. ICLR 2023 Oral [Towards Stable Test-Time Adaptation in Dynamic Wild World](https://openreview.net/forum?id=g2YraF75Tj)(æµ‹è¯•æ•°æ®æµå¯èƒ½å…·æœ‰æ··åˆåŸŸåç§»ã€å°æ‰¹é‡å’Œä¸å¹³è¡¡æ ‡ç­¾åˆ†å¸ƒshift)
16. ICLR 2023 [Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation](https://openreview.net/forum?id=FJXf1FXN8C)(å¸¦å…±è½­æ ‡ç­¾çš„GDæ”¶æ•›äºåœ¨é«˜æ–¯æ¨¡å‹ä¸‹ä»»æ„å°è¯¯å·®çš„æœ€ä¼˜é¢„æµ‹å™¨ï¼Œè€Œå¸¦æœ‰ä¼ ç»Ÿä¼ªæ ‡ç­¾çš„GDåœ¨æ­¤ä»»åŠ¡ä¸­å¤±è´¥ã€‚)
17. ICLR 2023  [Energy-Based Test Sample Adaptation for Domain Generalization](https://openreview.net/forum?id=3dnrKbeVatv)(å°†ç›®æ ‡åŸŸæ•°æ®adaptåˆ°æºåŸŸï¼Œåˆ©ç”¨éšæœºæ¢¯åº¦æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦çš„èƒ½é‡æœ€å°åŒ–æ–¹æ³•è¿­ä»£æ›´æ–°æ ·æœ¬)
18. ICLR 2023 [Deja Vu: Continual Model Generalization for Unseen Domains ](https://openreview.net/forum?id=L8iZdgeKmI6)(Continual Domain Shift Learning (CDSL))
19. ICLR 2023 [MECTA: Memory-Economic Continual Test-Time Model Adaptation](https://openreview.net/forum?id=N92hjSf5NNh)(ç›®å‰å¤§å¤šæ•°TTAæ–¹æ³•å†…å­˜æ¶ˆè€—æ¯”è¾ƒé«˜å› ä¸ºè¦åå‘ä¼ æ’­ï¼Œæœ¬æ–‡å»ºè®®å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œé‡‡ç”¨è‡ªé€‚åº”è§„èŒƒåŒ–å±‚æ¥ä¿æŒç¨³å®šå’Œå‡†ç¡®çš„é¢„æµ‹ï¼Œå¹¶å¯å‘å¼åœ°åœæ­¢åå‘ä¼ æ’­ç¼“å­˜ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯¹ç½‘ç»œè¿›è¡Œä¿®å‰ªä»¥å‡å°‘ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œå¹¶åœ¨ä¼˜åŒ–åæ¢å¤å‚æ•°ä»¥é¿å…é—å¿˜)
20. ICML 2023 Oral [The Price of Differential Privacy under Continual Observation](https://zhuanlan.zhihu.com/p/639174050)(è¿ç»­é€‚åº”æ¨¡å‹åœºæ™¯ä¸‹çš„å·®åˆ†éšç§)
21. ICML 2023 Oral [ODS: Test-Time Adaptation in the Presence of Open-World Data Shift](https://zhuanlan.zhihu.com/p/639174050)(åŒæ—¶é€‚åº”åå˜é‡å’Œæ ‡ç­¾åˆ†å¸ƒçš„åç§»)
22. ICML 2023 [Uncovering Adversarial Risks of Test-Time Adaptation](https://zhuanlan.zhihu.com/p/649295930)(æµ‹è¯•æ‰¹å¤„ç†ä¸­å¼•å…¥æ¶æ„æ ·æœ¬å¯èƒ½ä¼šå¯¹æœ€ç»ˆé¢„æµ‹æ¨¡å‹çš„ç”Ÿæˆäº§ç”Ÿå½±å“)
23. ICML 2023 [On Pitfalls of Test-Time Adaptation](https://zhuanlan.zhihu.com/p/649295930)(åä¸ºTTABçš„æµ‹è¯•æ—¶è‡ªé€‚åº”åŸºå‡†ï¼ŒåŒ…å«äº†åç§æœ€å…ˆè¿›çš„ç®—æ³•ã€å¤šæ ·åŒ–çš„åˆ†å¸ƒåç§»æƒ…å†µå’Œä¸¤ç§è¯„ä¼°åè®®ã€‚)
24. ICML 2023 [Leveraging Proxy of Training Data for Test-Time Adaptation](https://zhuanlan.zhihu.com/p/649295930)(ä½¿ç”¨è®­ç»ƒæ•°æ®çš„è½»é‡çº§ä¸”ä¿¡æ¯ä¸°å¯Œçš„ä»£ç†æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§å®Œå…¨åˆ©ç”¨è¿™äº›ä»£ç†çš„æµ‹è¯•é˜¶æ®µè‡ªé€‚åº”æ–¹æ³•)
25. ICML 2023 [Test-time Adaptation with Slot-Centric Models](https://zhuanlan.zhihu.com/p/649295930)(åœ¨åœºæ™¯åˆ†è§£ä»»åŠ¡ä¸­ï¼Œç®€å•çš„TTAæŸå¤±å¯¹äºä»»åŠ¡æ˜¯ä¸è¶³å¤Ÿçš„)
26. ICML 2023 [Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization](https://zhuanlan.zhihu.com/p/649295930)(å°†æµ‹è¯•æ ·æœ¬çš„æ ·å¼ï¼ˆä¸æºåŸŸå­˜åœ¨è¾ƒå¤§æ ·å¼å·®è·ï¼‰è½¬æ¢ä¸ºæ¨¡å‹å·²ç†Ÿæ‚‰çš„æœ€è¿‘çš„æºåŸŸæ ·å¼ï¼Œç„¶åè¿›è¡Œé¢„æµ‹)


# Robutness/Adaptation/Fairness/OOD Detection

1. ICML 2023 Oral [Delving into Noisy Label Detection with Clean Data](https://zhuanlan.zhihu.com/p/639174050)(å™ªå£°æ ‡ç­¾æ£€æµ‹ä¸­å¯¹å¹²å‡€æ•°æ®çš„åˆ©ç”¨ï¼Œä»¥æé«˜å™ªå£°æ ‡ç­¾æ£€æµ‹çš„æ€§èƒ½)

## 2022
1. Arxiv [Are Vision Transformers Robust to Spurious Correlations?](https://arxiv.org/pdf/2203.09125.pdf)(å¯¹ViTé²æ£’æ€§çš„ç ”ç©¶ï¼Œæ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒå‰æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¯¹ä¼ªç›¸å…³çš„é²æ£’æ€§ï¼Œé¢„è®­ç»ƒæ•°æ®è¾ƒå°‘åè€Œä¸å¦‚CNN)
2. CVPR [Exploring Domain-Invariant Parameters for Source FreeDomain Adaptation](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Exploring_Domain-Invariant_Parameters_for_Source_Free_Domain_Adaptation_CVPR_2022_paper.pdf)(ç›¸æ¯”äºä»¥å¾€å·¥ä½œæ¢ç´¢åŸŸä¸å˜ç‰¹å¾ï¼Œè¯¥å·¥ä½œæ—¨åœ¨å¯»æ‰¾åŸŸä¸å˜å‚æ•°)
3. CVPR [CENet: Consolidation-and-Exploration Network for Continuous DomainAdaptation](https://openaccess.thecvf.com/content/CVPR2022W/RoSe/papers/Zhang_CENet_Consolidation-and-Exploration_Network_for_Continuous_Domain_Adaptation_CVPRW_2022_paper.pdf)(æœ¬æ–‡è¯´ä»–æå‡ºäº†continuous DAçš„æ¦‚å¿µï¼Œä½†æ˜¯ICML 18å°±å·²ç»æå‡ºäº†å‘€ï¼Ÿ)
4. CVPR [Slimmable Domain Adaptation](https://arxiv.org/abs/2206.06620)(Adaptationçš„å¯¹è±¡ä¸ä»…åº”è¯¥æ˜¯æ•°æ®ï¼Œæœ¬æ–‡è€ƒè™‘ä¸‹æ¸¸è®¾å¤‡çš„adaptationã€‚)
5. NeurIPS Outstanding [Is Out-of-Distribution Detection Learnable?](https://arxiv.org/pdf/2210.14707.pdf)(å„ç§åœºæ™¯ä¸‹çš„OOD detectionçš„PACç†è®º)
6. ICML [Out-of-Distribution Detection with Deep Nearest Neighbors](https://arxiv.org/pdf/2204.06507.pdf)(ç”¨KNNåšOOD detection)
7. Arxiv [A Simple Test-Time Method for Out-of-Distribution Detection](https://arxiv.org/pdf/2207.08210.pdf)(test time adaptation for OOD detection)
8. Arxiv [RobArch: Designing Robust Architectures against Adversarial Attacks](https://shengyun-peng.github.io/papers/22_robarch.pdf)(å¯¹å¦‚ä½•è®¾è®¡é²æ£’æ€§æ›´å¼ºçš„æ¨¡å‹ç»“æ„åšäº†å¤§é‡çš„å®éªŒéªŒè¯)
9. ICCV [Nearest Neighbor Guidance for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf)(ç”¨IDæ•°æ®ç»™knn-oodå†è®¡ç®—ä¸€ä¸ªé¢å¤–çš„æ­£åˆ™é¡¹ï¼Œç¼“è§£knn-oodåœ¨near-IDé™„è¿‘è¡¨ç°ä¸å¥½çš„ç°è±¡)
10. ICCV [Understanding the Feature Norm for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Understanding_the_Feature_Norm_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf)( Feature Normä¸åˆ†ç±»å™¨ç½®ä¿¡åº¦æœ‰å…³ï¼Œä½†æ˜¯ä¸ä¾èµ–äºç±»åˆ«æ ‡ç­¾ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ— ç›‘ç£metric)

## before 2022
1. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
2. ICCV [Generalized Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/404697072)(ä¸ä½¿ç”¨æºåŸŸæ•°æ®ï¼Œåªæœ‰æºåŸŸé¢„è®­ç»ƒçš„æ¨¡å‹æ—¶å¦‚ä½•adaptationå¹¶ä¿è¯source domainçš„æ€§èƒ½)
3. ICCV [Adaptive Adversarial Network for Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(åœ¨æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èƒ½å¦å¯»æ‰¾ä¸€ç§æ–°çš„é’ˆå¯¹ç›®æ ‡çš„åˆ†ç±»å™¨ï¼Œå¹¶ä½¿å…¶é€‚åº”ç›®æ ‡ç‰¹å¾)
4. ICCV [Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(è¯¥ç®—æ³•é€šè¿‡ç‰¹å¾æå–å™¨å’Œé‰´åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—å­¦ä¹ æ¥å‡å°ç‰¹å¾æ¢¯åº¦åœ¨ä¸¤ä¸ªåŸŸä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚)
5. FAccT [Algorithmic recourse: from counterfactual explanations to interventions](https://zhuanlan.zhihu.com/p/424631782)(æå‡ºäº†causal recourseçš„æ¦‚å¿µ)
6. ICML WorkShop [On the Fairness of Causal Algorithmic Recourse](https://zhuanlan.zhihu.com/p/424631782)(æœ¬æ–‡åœ¨group recourseçš„åŸºç¡€ä¸Šè€ƒè™‘äº†å¤šä¸ªå˜é‡ä¹‹é—´çš„ç›¸äº’å½±å“å³æ‰€è°“çš„å› æœå…³ç³»ã€‚)
7. NeurIPS [Domain Adaptation with Invariant Representation Learning: What Transformations to Learn?](https://zhuanlan.zhihu.com/p/316265317)(DAä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªencoderï¼Ÿ)
8. NeurIPS [Gradual Domain Adaptation without Indexed Intermediate Domains](https://zhuanlan.zhihu.com/p/316265317)(æ²¡æœ‰domaparameterin labelçš„Gradual domain adaption(GDA))
9. NeurIPS [Implicit Semantic Response Alignment for Partial Domain Adaptation](https://zhuanlan.zhihu.com/p/316265317)(PDAå¦‚ä½•åˆ©ç”¨å¥½é¢å¤–ç±»)
10. NeurIPS [The balancing principle for parameter choice in distance-regularized domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(å¦‚ä½•æŒ‘é€‰åˆ†ç±»æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹çš„tradeoff parameter)
11. AAAI [Provable Guarantees for Understanding Out-of-distribution Detection](https://arxiv.org/pdf/2112.00787.pdf)(åŸºäºæ•°æ®æ˜¯é«˜æ–¯æ··åˆçš„å‡è®¾ç»™å‡ºæœ€ä¼˜densityä¼°è®¡æ–¹å¼)
12. Available at Optimization Online [Kullback-Leibler Divergence Constrained Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(å¼€ç¯‡ä¹‹ä½œï¼Œä½¿ç”¨KLæ•£åº¦æ„é€ DROä¸­çš„uncertainty set)
13. ICLR 2018 Oral [Certifying Some Distributional Robustnesswith Principled Adversarial Training](https://zhuanlan.zhihu.com/p/381176721)(åŸºäº Wasserstein-ballæ„é€ uncertainty setï¼Œç”¨äºadversarial robustness)
14. ICML 2018 Oral [Does Distributionally Robust Supervised Learning Give Robust Classifiers?](https://zhuanlan.zhihu.com/p/381176721)(DROå°±ä¸€å®šæ¯”ERMå¥½ï¼Ÿä¸ä¸€å®šï¼å¿…é¡»å¼•å…¥é¢å¤–ä¿¡æ¯)
15. NeurIPS 2019 [Distributionally Robust Optimization and Generalization in Kernel Methods](https://zhuanlan.zhihu.com/p/381176721)(æœ¬æ–‡ä½¿ç”¨MMD(maximummean discrepancy)å¯¹uncertainty setè¿›è¡Œå»ºæ¨¡ï¼Œå¾—åˆ°äº†MMD DRO)
16. EMNLP 2019 [Distributionally Robust Language Modeling](https://zhuanlan.zhihu.com/p/381176721)(Coarse-grained mixture modelsåœ¨NLPä¸­çš„ç»å…¸æ¡ˆä¾‹)
17. Arxiv 2019 [Equalizing recourse across groups](https://zhuanlan.zhihu.com/p/424631782)(åŸºç¡€çš„recourseæµ‹é‡çš„æ˜¯å•ä¸ªæ ·æœ¬ï¼Œæœ¬æ–‡ç»™å‡ºäº†ä¸€ä¸ªgroupçº§åˆ«çš„recourseåº¦é‡ã€‚)
18. ICML 2020 Oral [Continuously indexed domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(è¿ç»­å˜åŒ–çš„domain)

# Data-Centric/Prompt/Large-Pretrain-Model

## Data Centric
1. AISTATS 2019 [Towards Optimal Transport with Global Invariances](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å¯¹é½ä¸¤ä¸ªæ•°æ®é›†ï¼Ÿ)
2. NeurIPS 2020 [Geometric Dataset Distances via Optimal Transport](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å®šä¹‰ä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´çš„è·ç¦»ï¼Ÿ)
3. ICML 2021 [Dataset Dynamics via Gradient Flows in Probability Space](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•è¿›è¡Œæ•°æ®é›†ä¼˜åŒ–ï¼Œä½¿å¾—ä¸¤ä¸ªæ•°æ®é›†å°½å¯èƒ½çš„åƒï¼Ÿ)

## Prompts

1. ACL 2021 [WARP: Word-level Adversarial ReProgramming](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptå¼€ç¯‡ä¹‹ä½œ)
2. Arxiv 2021 **Stanford**[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptç”¨äºNLGçš„å„ç§ä»»åŠ¡)(å°†promptç”¨äºNLGä»»åŠ¡ä¸Š)
3. Arxiv 2021 **Google**[The Power of Scale for Parameter-Efficient Prompt Tuning](https://zhuanlan.zhihu.com/p/407144573)(ç›®å‰æœ€ç®€å•çš„preifx trainingï¼šåªå¯¹inputæ·»åŠ prefix)
4. Arxiv 2021 **DeepMind**[Multimodal Few-Shot Learning with Frozen Language Models](https://zhuanlan.zhihu.com/p/407144573)(åˆ©ç”¨å›¾åƒç¼–ç å™¨æŠŠå›¾åƒä½œä¸ºä¸€ç§åŠ¨æ€çš„prefixï¼Œä¸æ–‡æœ¬ä¸€èµ·é€å…¥LMä¸­)

## Large-Pretrain-Model

1. ICML 2023 Oral [Scaling Vision Transformers to 22 Billion Parameters](https://zhuanlan.zhihu.com/p/639174050)(å°†ViTæ‰©å±•åˆ°220äº¿çš„å‚æ•°é‡)
2. ICML 2023 Oral [Specializing Smaller Language Models towards Multi-Step Reasoning](https://zhuanlan.zhihu.com/p/639174050)(é€šè¿‡è’¸é¦é™ä½å¤§æ¨¡å‹çš„å…¨é¢æ€§ï¼Œæå‡å°æ¨¡å‹çš„ä¸“ä¸šæ€§)
3. ICML 2023 Oral [Pretraining Language Models with Human Preferences](https://zhuanlan.zhihu.com/p/639174050)(ç»ƒä¸€å¼€å§‹å°±çº³å…¥äººç±»åå¥½è€Œä¸æ˜¯finetuneæ—¶)
4. ICML 2023 Oral [Whose Opinions Do Language Models Reflect?](https://zhuanlan.zhihu.com/p/639174050)(LMsåæ˜ çš„è§‚ç‚¹ä¸ç¾å›½äººå£ç¾¤ä½“å­˜åœ¨ç›¸å½“å¤§çš„ä¸ä¸€è‡´)
5. ICML 2023 Oral [Mimetic Initialization of Self-Attention Layers](https://zhuanlan.zhihu.com/p/639174050)(mimetic initializationè®©attentionåœ¨å°æ•°æ®é›†ä¸Šä¹Ÿèƒ½è®­ç»ƒçš„å¾ˆå¥½)
6. ICML 2023 Oral [Cross-Modal Fine-Tuning: Align then Refine](https://zhuanlan.zhihu.com/p/639174050)(è·¨æ¨¡æ€å¾®è°ƒæ¡†æ¶ï¼Œå°†å•ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„é€‚ç”¨æ€§æ‰©å±•åˆ°å¤šæ ·çš„æ¨¡æ€)
7. ICML 2023 Oral [Evaluating Self-Supervised Learning via Risk Decomposition](https://zhuanlan.zhihu.com/p/639174050)(ç»¼åˆæŒ‡æ ‡è¯„ä¼°è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ€§èƒ½)
8. ICML 2023 [XTab: Cross-table Pretraining for Tabular Transformers](https://zhuanlan.zhihu.com/p/614276528)(åœ¨å„ä¸ªé¢†åŸŸçš„ä¸åŒæ•°æ®è¡¨ä¸Šè¿›è¡Œè·¨è¡¨é¢„è®­ç»ƒ)
9. ICML 2023 [Multi-Environment Pretraining Enables Transfer to Action Limited Datasets](https://zhuanlan.zhihu.com/p/614276528)(è§£å†³å¤§è§„æ¨¡è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„åºåˆ—å†³ç­–æ•°æ®å¾€å¾€ç¼ºä¹æ ‡æ³¨çš„åŠ¨ä½œä¿¡æ¯)
10. ICML 2023 [Why do Nearest Neighbor Language Models Work?](https://zhuanlan.zhihu.com/p/614276528)(åˆ†ækNN-LMä¸ä¼ ç»ŸLMçš„ä¸åŒç»´åº¦æ¥å›ç­”kNN-LMä¸ºä»€ä¹ˆå¥½)
11. ICML 2023 [PaLM-E: An Embodied Multimodal Language Model](https://zhuanlan.zhihu.com/p/614276528)(å°†çœŸå®ä¸–ç•Œä¸­çš„è¿ç»­ä¼ æ„Ÿå™¨æ¨¡æ€ç›´æ¥èå…¥è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œå»ºç«‹å•è¯å’Œæ„ŸçŸ¥ä¹‹é—´çš„è”ç³»)
12. ICML 2023 [Compositional Exemplars for In-context Learning](https://zhuanlan.zhihu.com/p/614276528)(å¦‚ä½•æ”¹è¿›ç°æœ‰çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©æ–¹æ³•)
13. ICML 2023 [Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models](https://zhuanlan.zhihu.com/p/614276528)(åˆ©ç”¨äººå·¥åˆ›å»ºçš„ä¸€äº›ç¤ºä¾‹æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ›´å¤šç¤ºä¾‹ï¼Œå¹¶é€‰æ‹©æœ‰æ•ˆçš„ç¤ºä¾‹ä»¥ä¿ƒè¿›æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚)

# Optimization/GNN/Energy/Generative/Causality/Others

## Optimization
1. ICML 2021 [An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming](https://zhuanlan.zhihu.com/p/390808626)
2. NeurIPS 2021 _Deep Structural Causal Models for Tractable Counterfactual Inference_
1. ICML 2018 _Bilevel Programming for Hyperparameter Optimization and Meta-Learning_(ç”¨bi-level programmingå»ºæ¨¡è¶…å‚æ•°æœç´¢ä¸meta-learning)
2. NeurIPS 2021 [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)

## Individual Treatment Estimation
1. ICML 2017 [Estimating individual treatment effect: generalization bounds and algorithms](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡ç¬¬ä¸€æ¬¡æå‡ºäº†ITEçš„æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨DAçš„ä¸€å¥—ç†è®ºå¯¹å…¶è¿›è¡Œboundï¼Œä¾æ¬¡è®¾è®¡äº†ä¸€å¥—è¡Œè€Œæœ‰æ•ˆçš„ç®—æ³•ã€‚)
2. NeurIPS 2019 [Adapting Neural Networks for the Estimation of Treatment Effects](https://zhuanlan.zhihu.com/p/426793887)(è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³æ˜¯è¿™æ ·çš„ï¼šæˆ‘ä»¬æ²¡å¿…è¦ä½¿ç”¨æ‰€æœ‰çš„åæ–¹å·®å˜é‡Xè¿›è¡Œadjustmentã€‚)
3. PNAS 2019 [Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶X-learnerï¼Œå½“å„ä¸ªtreatmentç»„çš„æ•°æ®éå¸¸ä¸å‡è¡¡çš„æ—¶å€™ï¼Œè¿™ç§æ¡†æ¶éå¸¸æœ‰æ•ˆã€‚)
4. AAAI 2020 [Learning Counterfactual Representations for Estimating Individual Dose-Response Curves](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†æ–°çš„metricï¼Œæ–°çš„æ•°æ®é›†ï¼Œå’Œè®­ç»ƒç­–ç•¥ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„treatmentçš„outcomeè¿›è¡Œä¼°è®¡ã€‚)
5. ICLR 2021 Oral: [VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡åŸºäºvarying coefficient modelï¼Œè®©æ¯ä¸ªtreatmentå¯¹åº”çš„branchæˆä¸ºtreatmentçš„å‡½æ•°ï¼Œè€Œä¸éœ€è¦å•ç‹¬è®¾è®¡branchï¼Œä¾æ¬¡è¾¾åˆ°çœŸæ­£çš„è¿ç»­æ€§ã€‚)
6. Arxiv 2021 [Neural Counterfactual Representation Learning for Combinations of Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡è€ƒè™‘æ›´å¤æ‚çš„æƒ…å†µï¼šå¤šç§treatmentå…±åŒä½œç”¨ã€‚)
7. NeurIPS 2021 Spotlight [On Inductive Biases for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2106.03765)(æœ¬æ–‡æå‡ºäº†æ–°æ¡†æ¶FlexTENetï¼Œç›´æ¥å¯¹æ¡ä»¶å› æœå€¼Ï„è¿›è¡Œä¼°è®¡ï¼Œè€Œä¸æ˜¯å¯¹Î¼1ï¼ŒÎ¼2åˆ†åˆ«ä¼°è®¡)
8. NeurIPS 2021 [Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms](https://arxiv.org/abs/2101.10943)(æœ¬æ–‡åˆ†æäº†è¿›æ¥è¿›è¡Œ individual treatment effectçš„å„ç§ç®—æ³•èŒƒå¼ï¼Œ)
9. Arxiv 2021 [Cycle-Balanced Representation Learning For Counterfactual Inference](å¯¹treatmentï¼Œcontrolä¸¤ä¸ªgroupåˆ†åˆ«encodeï¼Œç„¶åå¯¹æŠ—å­¦ä¹ å‡å°‘åŸŸå·®è·ï¼Œä¸ºäº†é˜²æ­¢åˆ†ç±»ä¿¡æ¯è¢«æŠ¹å»ï¼ŒåŠ ä¸Šcycle-consistanceçš„çº¦æŸï¼Œé‡æ„ç‰¹å¾ã€‚)


## LTH (Lottery Ticket Hypothesis)
1. NeurIPS 2020: [The Lottery Ticket Hypothesis for Pre-trained BERT Networks](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºBERT fine-tune))
2. ICML 2021 Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºOODæ³›åŒ–)
3. CVPR 2021: [The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºè§†è§‰æ¨¡å‹é¢„è®­ç»ƒ)


## Generative Model (mainly diffusion model)
1. _Estimation of Non-Normalized Statistical Models by Score Matching_(ä½¿ç”¨åˆ†æ­¥ç§¯åˆ†ï¼ˆScore Matchingï¼‰çš„æ–¹æ³•è§£å†³éå½’ä¸€åŒ–åˆ†å¸ƒçš„ä¼°è®¡é—®é¢˜)
2. UAI 2019 _Sliced Score Matching: A Scalable Approach to Density and Score Estimation_(å°†é«˜ç»´çš„æ¢¯åº¦åœºæ²¿éšå³æ–¹å‘æŠ•å½±åˆ°ä¸€ç»´çš„æ ‡é‡åœºå†è¿›è¡Œscore-macthing) 
3. NeurIPS 2019 Oral _Generative Modeling by Estimating Gradients of the Data Distribution_(é€šè¿‡æ·»åŠ å™ªå£°çš„æ–¹æ³•ï¼Œå¢å¼ºLangevin MCMCå¯¹ä½æ¦‚ç‡å¯†åº¦åŒºåŸŸçš„å»ºæ¨¡èƒ½åŠ›)
4. NeurIPS 2020 _improved techniques for training score-based generative models_(å¯¹score-based generative modelå¤±è´¥æ¡ˆä¾‹çš„åˆ†æå’Œæ”¹è¿›ï¼Œç”Ÿæˆèƒ½åŠ›å¼€å§‹åª²ç¾GAN)
6. NeurIPS 2020 [Denoising Diffusion Probabilistic Models](https://zhuanlan.zhihu.com/p/366004028)(é™¤VAE, GAN, FLOWå¤–åˆä¸€ç”ŸæˆèŒƒå¼)
7. ICLR 2021 **Outstanding Paper Award** [Score-Based Generative Modeling through Stochastic Differential Equations](http://yang-song.github.io/blog/2021/score/)
8. Arxiv 2021 _Diffusion Models Beat GANs on Image Synthesis_(Diffusion Modelsåœ¨å›¾åƒå’Œåˆæˆä¸Šè¶…è¶ŠGAN) 
10. Arxiv 2021 Variational Diffusion Models

## Implicit Neural Representation (INR)
1. NeurIPS 2020 (Oral)ï¼š [Implicit Neural Representations with Periodic Activation Functions](https://zhuanlan.zhihu.com/p/472942119)
2. SIGGRAPH Asia 2020ï¼š [X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation](https://zhuanlan.zhihu.com/p/472942119)
3. CVPR 2021 (Oral)ï¼š[Learning Continuous Image Representation with Local Implicit Image Function](https://zhuanlan.zhihu.com/p/472942119)
4. CVPR 2021 [Adversarial Generation of Continuous Images](https://arxiv.org/abs/2011.12026)
5. NeurIPS 2021 [Learning Signal-Agnostic Manifolds of Neural Fields](https://arxiv.org/abs/2111.06387)
6. Arxiv 2021 [Generative Models as Distributions of Functions](https://arxiv.org/abs/2102.04776)


## Survey
1. [ç»¼è¿°ï¼šåŸºäºèƒ½é‡çš„æ¨¡å‹](https://zhuanlan.zhihu.com/p/343529491)
