

This is a repository for organizing articles related to Domain generalization, OOD, optimization, data-centric learning, prompt learning, robutness, and causality. Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.

### About Me :high_brightness: 
I'm the first year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/).


###  ğŸ”¥ Updated 2022-3-26
- Our paper Towards Principled Disentanglement for Domain Generalization is accepted to CVPR2022. :blush: [[Reading Notes]](https://zhuanlan.zhihu.com/p/477855079) [[Code]](https://github.com/hlzhang109/DDG)  [[paper]](https://arxiv.org/abs/2111.13839)
- Recent Domain generalization/OOD/Robustness papers on Arxiv have been updated.
- Implicit Neural Representation (INR) papers on 2D images have been updated.

# Table of Contents (ongoing)
* [Generalization/OOD](#generalizationood)
   * [2022](#2022)
   * [2021](#2021)
   * [2020](#2020)
   * [OLD but Important](#old-but-important)
   * [Survey](#survey)
* [Robutness/Adaptation/Fairness](#robutnessadaptationfairness)
   * [2022](#2022-1)
   * [2021](#2021-1)
   * [Before 2021](#before-2021)
* [Causality](#causality)
   * [Individual Treatment Estimation](#individual-treatment-estimation)
* [Data-Centric/Prompt](#data-centricprompt)
   * [Data Centric](#data-centric)
   * [Prompts](#prompts)
* [Optimization/GNN/Energy/Others](#optimizationgnnenergygenerativeothers)
   * [Optimization](#optimization)
   * [LTH (Lottery Ticket Hypothesis)](#lth-lottery-ticket-hypothesis)
   * [Generative Model (Mainly Diffusion Model)](#generative-model-mainly-diffusion-model)
   * [Implicit Neural Representation (INR)](#implicit-neural-representation-inr)
   * [Survey](#survey-1)
# Generalization/OOD
## 2022
1. CVPRï¼ˆCMUï¼‰ [Towards Principled Disentanglement for Domain Generalization](https://zhuanlan.zhihu.com/p/477855079)(å°†è§£è€¦ç”¨äºDGï¼Œæ–°ç†è®ºï¼Œæ–°æ–¹æ³•)
2. Arxiv [WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks](https://woods-benchmarks.github.io/)(ä¸€ä¸ªå…³äºæ—¶åºæ•°æ®OODçš„å¤šä¸ªbenchmark)
3. Arxiv [A Broad Study of Pre-training for DomainGeneralization and Adaptation](https://arxiv.org/pdf/2203.11819.pdf)(æ·±å…¥ç ”ç©¶äº†é¢„è®­ç»ƒå¯¹äºDA,DGä»»åŠ¡çš„ä½œç”¨ï¼Œç®€å•çš„ä½¿ç”¨ç›®å‰æœ€å¥½çš„backboneè¶³å·²å–å¾—SOTAçš„æ•ˆæœ)
4. Arxiv [Domain Generalization by Mutual-InformationRegularization with Pre-trained Models](https://arxiv.org/pdf/2203.10789.pdf)(ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æŒ‡å¯¼finetuneçš„è¿‡ç¨‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›)
5. ICLR Oral [A Fine-Grained Analysis on Distribution Shift](https://zhuanlan.zhihu.com/p/466675818)(å¦‚ä½•å‡†ç¡®çš„å®šä¹‰distribution shiftï¼Œä»¥åŠå¦‚ä½•ç³»ç»Ÿçš„æµ‹é‡æ¨¡å‹çš„é²æ£’æ€§)
6. ICLR Oral [Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution](https://zhuanlan.zhihu.com/p/466675818)(fine-tuningï¼ˆå¾®è°ƒï¼‰å’Œlinear probingç›¸è¾…ç›¸æˆ)
7. ICLR Spotlight [Towards a Unified View of Parameter-Efficient Transfer Learning](https://zhuanlan.zhihu.com/p/466675818)(ç»Ÿä¸€çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç†è®ºæ¡†æ¶)
8. ICLR Spotlight [How Do Vision Transformers Work?](https://zhuanlan.zhihu.com/p/466675818)(Vision Transformers (ViTs)çš„ä¼˜è‰¯ç‰¹æ€§)
9. ICLR Spotlight [On Predicting Generalization using GANs](https://zhuanlan.zhihu.com/p/466675818)(ä½¿ç”¨æºåŸŸæ•°æ®è®­ç»ƒå‡ºçš„GANæ¥é¢„æµ‹æµ‹è¯•è¯¯å·®)
10. ICLR Poster [Uncertainty Modeling for Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/466675818)(åŸŸæ³›åŒ–æ—¶è€ƒè™‘ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•)
11. ICLR Poster [Gradient Matching for Domain Generalization](https://zhuanlan.zhihu.com/p/466675818)(é¼“åŠ±æ¥è‡ªä¸åŒåŸŸçš„æ¢¯åº¦ä¹‹é—´çš„å†…ç§¯æ›´å¤§)
## 2021
1. ICML [Improved OOD Generalization via Adversarial Training and Pre-training](https://proceedings.mlr.press/v139/yi21a.html)(ä»ç†è®ºä¸Šè¡¨æ˜ï¼Œä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œé‚£ä¹ˆå¯¹ä¸‹æ¸¸OODæ•°æ®çš„æ³›åŒ–å¯ä»¥æä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚)
2. ICCV [CrossNorm and SelfNorm for Generalization under Distribution Shifts](https://zhuanlan.zhihu.com/p/426728622)(æ€è·¯ç®€å•çš„æ­£åˆ™åŒ–æŠ€æœ¯ç”¨äºDG)
3. ICCV [A Style and Semantic Memory Mechanism for Domain Generalization](https://zhuanlan.zhihu.com/p/426728622)(å°è¯•ç€å»ä½¿ç”¨intra-domain style invarianceæ¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½)
4. Arxiv: [Towards a Theoretical Framework of Out-of-Distribution Generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆæ–°ç†è®ºï¼‰
5. Arxiv(**Yoshua Bengio**) _Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization_ (å½“OODé‡åˆ°ä¿¡æ¯ç“¶é¢ˆç†è®º)
6. Arxiv _Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation_
7. Arxiv _Embracing the Dark Knowledge: Domain Generalization Using Regularized Knowledge Distillation_(ä½¿ç”¨çŸ¥è¯†è’¸é¦ä½œä¸ºæ­£åˆ™åŒ–æ‰‹æ®µ)
8. Arxiv _Delving Deep into the Generalization of Vision Transformers under Distribution Shifts_ (è§†è§‰transformerçš„æ³›åŒ–æ€§è®¨è®º)
9. Arxiv _Training Data Subset Selection for Regression with Controlled Generalization Error_ (ä»å¤§é‡è®­ç»ƒå®ä¾‹ä¸­é€‰æ‹©æ•°æ®å­é›†ï¼Œå¹¶ä¿æŒå¯æ¯”çš„æ³›åŒ–æ€§)
10. Arxiv(**MIT**) _Measuring Generalization with Optimal Transport_ (ç½‘ç»œå¤æ‚åº¦ä¸æ³›åŒ–æ€§çš„ç†è®ºç ”ç©¶ï¼Œ)
11. Arxiv(**SJTU**) [OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms](https://view.inews.qq.com/a/20210615A04V1C00?tbkt=B1&uid=) (æ­ç¤ºOODçš„è¯„æµ‹æ ‡å‡†å°šä¸å®Œå–„å¹¶æå‡ºè¯„æµ‹æ–¹æ¡ˆ)
12. Arxiv (Tsinghu) _Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization_ (æ–°çš„taskï¼šæ— ç›‘ç£çš„DGï¼ŒæºåŸŸçš„æ•°æ®æ ‡ç­¾ä¸å¯ä»¥ç”¨)
13. ICML Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆå½©ç¥¨æ¨¡å‹å¯»æ‰¾æ¨¡å‹ä¸­æ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„å°æ¨¡å‹ï¼‰
14. ICML Oralï¼š[Domain Generalization using Causal Matching](https://zhuanlan.zhihu.com/p/382608823) (contrastive lossç‰¹å¾å¯¹é½+ç‰¹å¾ä¸å˜æ€§çº¦æŸ)
15. ICML Oral: _Just Train Twice: Improving Group Robustness without Training Group Information_
16. ICML Spotlight: [Environment Inference for Invariant Learning](https://zhuanlan.zhihu.com/p/382608823) (æ²¡æœ‰åŸŸæ ‡ç­¾å¦‚ä½•å­¦ä¹ åŸŸä¸å˜æ€§ç‰¹å¾ï¼Ÿ)
17. ICLR Poster: [Understanding the failure modes of out-of-distribution generalization](https://zhuanlan.zhihu.com/p/382608823) ï¼ˆOODå¤±è´¥çš„ä¸¤ç§åŸå› ï¼‰
18. ICLR Poster: [An Empirical Study of Invariant Risk Minimization](https://openreview.net/forum?id=jrA5GAccy_)(å¯¹IRMçš„å®éªŒæ€§æ¢ç´¢ï¼Œå¦‚å¯è§åŸŸçš„diversityå¦‚ä½•å½±å“IRMæ€§èƒ½ç­‰)
19. ICLR Poster _In Search of Lost Domain Generalization_ (æ²¡æœ‰model selectionçš„æ–¹æ³•ä¸æ˜¯å¥½æ–¹æ³•ï¼Œå¦‚ä½•æ ¹æ®éªŒè¯é›†é€‰æ‹©æ¨¡å‹ï¼Ÿ)
20. ICLR Poster [Modeling the Second Player in Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(ç”¨å¯¹æŠ—å­¦ä¹ å»ºæ¨¡DROçš„uncertainty set)
21. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
22. ICLR Spotlight(**Yoshua Bengio**) [Systematic generalisation with group invariant predictions](https://zhuanlan.zhihu.com/p/382608823) (å°†æ¯ä¸ªç±»åˆ†æˆä¸åŒçš„domain(_environment inference_ï¼Œç„¶åçº¦æŸæ¯ä¸ªåŸŸçš„ç‰¹å¾å°½å¯èƒ½ä¸€è‡´ä»è€Œé¿å…è™šå‡ä¾èµ–))
23. CVPR Oral: [Reducing Domain Gap by Reducing Style Bias](https://zhuanlan.zhihu.com/p/382608823) (channel-wise å‡å€¼ä½œä¸ºå›¾åƒé£æ ¼ï¼Œå‡å°‘CNNå¯¹é£æ ¼çš„ä¾èµ–)
24. AISTATS _Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions_
25. AISTATS Oral _Does Invariant Risk Minimization Capture Invariance_(IRMåªæœ‰åœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„æƒ…å†µä¸‹æ‰èƒ½çœŸæ­£æ•æ‰ä¸å˜å½¢ç‰¹å¾)
26. NeurIPS [Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests](https://arxiv.org/abs/2106.00545)(æœ¬æ–‡ä½¿ç”¨å› æœå·¥å…·è®¾è®¡äº†ä¸€ä¸ªå¯è¡Œçš„ç®—æ³•ï¼Œå°†åäº‹å®æ¨ç†ä¸åŸŸæ³›åŒ–ï¼ˆOODï¼‰è”ç³»èµ·æ¥ï¼Œè¿›è¡Œæœ‰æ•ˆçš„â€œstress testâ€ï¼Œæ¯”å¦‚å˜åŒ–ä¸€ä¸ªå¥å­åŒ…å«çš„çš„genderä¿¡æ¯ï¼Œçœ‹æœ€åæƒ…æ„Ÿåˆ†ç±»ä¼šä¸ä¼šæ”¹å˜ã€‚)
27. NeurIPS [Adaptive Risk Minimization: Learning to Adapt to Domain Shift](https://zhuanlan.zhihu.com/p/357962431)(åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®æ¥æ›´å¥½åœ°å¤„ç†æ–°domainå¼•èµ·çš„distribution shift)
28. NeurIPS [An Empirical Investigation of Domain Generalization with Empirical Risk Minimizers](https://zhuanlan.zhihu.com/p/357962431)(åŸºäºdomain adaptationçš„ç†è®ºæµ‹é‡æ–¹æ³•ä¸èƒ½å‡†ç¡®åœ°æ•æ‰OODæ³›åŒ–è¡Œä¸º)
29. NeurIPS Spotlight [On Inductive Biases for Heterogeneous Treatment Effect Estimation](https://zhuanlan.zhihu.com/p/357962431)(ä½¿ç”¨å› æœå·¥å…·è®¾è®¡äº†ä¸€ä¸ªå¯è¡Œçš„ç®—æ³•ï¼Œå°†åäº‹å®æ¨ç†ä¸åŸŸæ³›åŒ–ï¼ˆOODï¼‰è”ç³»èµ·æ¥)
30. NeurIPS Spotlight [Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization](https://zhuanlan.zhihu.com/p/357962431)(åœ¨testçš„é˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°æ¨¡å‹å¤´éƒ¨çš„linearå±‚)
31. NeurIPS [Why Do Better Loss Functions Lead to Less Transferable Features?](https://zhuanlan.zhihu.com/p/357962431)(æœ¬æ–‡ç ”ç©¶äº†è®­ç»ƒç›®æ ‡çš„é€‰æ‹©å¦‚ä½•å½±å“å·ç§¯ç¥ç»ç½‘ç»œåœ¨ImageNetä¸Šè®­ç»ƒå¾—åˆ°çš„å¯è¿ç§»æ€§)

## 2020
1. Arxiv [I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models](https://zhuanlan.zhihu.com/p/288980706)(å°†Domain Adaptationçœ‹ä½œæ˜¯å› æœå›¾æ¨ç†é—®é¢˜)
2. Arxiv (**Stanford**)_Distributionally Robust Lossesfor Latent Covariate Mixtures_.
3. NeurIPS [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)(ä½¿ç”¨èƒ½é‡æ¨¡å‹æ£€æµ‹OODæ ·æœ¬)
4. NeurIPS _Fairness without demographics through adversarially reweighted learning_ (åˆ©ç”¨å¯¹æŠ—å­¦ä¹ å¯¹éš¾æ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œå¸Œæœ›åŠ æƒåçš„æ ·æœ¬ä½¿å¾—åˆ†ç±»å™¨çš„æŸå¤±æ›´å¤§)
5. NeurIPS _Self-training Avoids Using Spurious Features Under Domain Shift_ (ä½¿ç”¨target domainçš„æ— æ ‡ç­¾æ•°æ®è®­ç»ƒæœ‰åŠ©äºé¿å…ä½¿ç”¨è™šå‡ç‰¹å¾)
6. NeurIPS _What shapes feature representations? Exploring datasets, architectures, and training_(Simplicity Biasï¼Œç¥ç»ç½‘ç»œå€¾å‘äºæ‹Ÿåˆâ€œå®¹æ˜“â€çš„ç‰¹å¾)
7. Arxiv [Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (å¥ åŸºä¹‹ä½œï¼Œè·³å‡ºç»éªŒé£é™©æœ€å°åŒ–--ä¸å˜é£é™©æœ€å°åŒ–)
8. ICLR Poster [The Risks of Invariant Risk Minimization](https://zhuanlan.zhihu.com/p/273209891) (ä¸å˜é£é™©æœ€å°åŒ–çš„ç¼ºé™·:åŸŸæ•°ç›®è¿‡å°‘IRMå³å¤±è´¥)
9. ICLR _Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization_(GroupDRO: æ‹¥æœ‰å¼ºæ­£åˆ™çš„DRO)
10. ICML _An investigation of why overparameterizationexacerbates spurious correlations_(ç¥ç»ç½‘ç»œçš„è¿‡å‚æ•°åŒ–æ˜¯é€ æˆç½‘ç»œä½¿ç”¨è™šå‡ç›¸å…³æ€§çš„é‡è¦åŸå› )
11. ICML UDA workshop _Learning Robust Representations with Score Invariant Learning_(éå½’ä¸€åŒ–ç»Ÿè®¡æ¨¡å‹ï¼šç”¨èƒ½é‡å­¦ä¹ çš„æ–¹å¼åšOOD)

## OLD but Important
1. ICML 2018 Oral (**Stanford**) _Fairness Without Demographics in Repeated Loss Minimization._
2. ICCV 2017 [CCSA--Unified Deep Supervised Domain Adaptation and Generalization](https://blog.csdn.net/Adupanfei/article/details/85165667) (å¯¹æ¯”æŸå¤±å¯¹é½æºåŸŸç›®æ ‡åŸŸæ ·æœ¬ç©ºé—´)
3. JSTOR (**Peters**)Causal inference by using invariant prediction: identification and confidence intervals.
4. ICML 2015 [Towards a Learning Theory of Cause-Effect Inference](ä½¿ç”¨kernel mean embeddingå’Œåˆ†ç±»å™¨è¿›è¡Œcasual inference                  )
5. IJCAI 2020 (**CMU**) _Causal Discovery from Heterogeneous/Nonstationary Data_

## Survey
1. [Causality åŸºç¡€æ¦‚å¿µæ±‡æ€»](https://zhuanlan.zhihu.com/p/269625734)
2. [Domain AdaptationåŸºç¡€æ¦‚å¿µä¸ç›¸å…³æ–‡ç« è§£è¯»](https://zhuanlan.zhihu.com/p/272508224)


# Robutness/Adaptation/Fairness
## 2022
1. Arxiv [Are Vision Transformers Robust to Spurious Correlations?](https://arxiv.org/pdf/2203.09125.pdf)(å¯¹ViTé²æ£’æ€§çš„ç ”ç©¶ï¼Œæ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒå‰æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜å¯¹ä¼ªç›¸å…³çš„é²æ£’æ€§ï¼Œé¢„è®­ç»ƒæ•°æ®è¾ƒå°‘åè€Œä¸å¦‚CNN)

## 2021
1. ICLR Poster [Learning perturbation sets for robust machine learning](https://zhuanlan.zhihu.com/p/391235069)(ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ‰°åŠ¨é›†åˆ)
2. ICCV [Generalized Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/404697072)(ä¸ä½¿ç”¨æºåŸŸæ•°æ®ï¼Œåªæœ‰æºåŸŸé¢„è®­ç»ƒçš„æ¨¡å‹æ—¶å¦‚ä½•adaptationå¹¶ä¿è¯source domainçš„æ€§èƒ½)
3. ICCV [Adaptive Adversarial Network for Source-free Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(åœ¨æ¨¡å‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èƒ½å¦å¯»æ‰¾ä¸€ç§æ–°çš„é’ˆå¯¹ç›®æ ‡çš„åˆ†ç±»å™¨ï¼Œå¹¶ä½¿å…¶é€‚åº”ç›®æ ‡ç‰¹å¾)
4. ICCV [Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation](https://zhuanlan.zhihu.com/p/426728622)(è¯¥ç®—æ³•é€šè¿‡ç‰¹å¾æå–å™¨å’Œé‰´åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—å­¦ä¹ æ¥å‡å°ç‰¹å¾æ¢¯åº¦åœ¨ä¸¤ä¸ªåŸŸä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚)
5. FAccT [Algorithmic recourse: from counterfactual explanations to interventions](https://zhuanlan.zhihu.com/p/424631782)(æå‡ºäº†causal recourseçš„æ¦‚å¿µ)
6. ICML WorkShop [On the Fairness of Causal Algorithmic Recourse](https://zhuanlan.zhihu.com/p/424631782)(æœ¬æ–‡åœ¨group recourseçš„åŸºç¡€ä¸Šè€ƒè™‘äº†å¤šä¸ªå˜é‡ä¹‹é—´çš„ç›¸äº’å½±å“å³æ‰€è°“çš„å› æœå…³ç³»ã€‚)
7. NeurIPS [Domain Adaptation with Invariant Representation Learning: What Transformations to Learn?](https://zhuanlan.zhihu.com/p/316265317)(DAä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªencoderï¼Ÿ)
8. NeurIPS [Gradual Domain Adaptation without Indexed Intermediate Domains](https://zhuanlan.zhihu.com/p/316265317)(æ²¡æœ‰domaparameterin labelçš„Gradual domain adaption(GDA))
9. NeurIPS [Implicit Semantic Response Alignment for Partial Domain Adaptation](https://zhuanlan.zhihu.com/p/316265317)(PDAå¦‚ä½•åˆ©ç”¨å¥½é¢å¤–ç±»)
10. NeurIPS [The balancing principle for parameter choice in distance-regularized domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(å¦‚ä½•æŒ‘é€‰åˆ†ç±»æŸå¤±å’Œæ­£åˆ™åŒ–é¡¹çš„tradeoff parameter)


## Before 2021
1. Available at Optimization Online [Kullback-Leibler Divergence Constrained Distributionally Robust Optimization](https://zhuanlan.zhihu.com/p/381176721)(å¼€ç¯‡ä¹‹ä½œï¼Œä½¿ç”¨KLæ•£åº¦æ„é€ DROä¸­çš„uncertainty set)
2. ICLR 2018 Oral [Certifying Some Distributional Robustnesswith Principled Adversarial Training](https://zhuanlan.zhihu.com/p/381176721)(åŸºäº Wasserstein-ballæ„é€ uncertainty setï¼Œç”¨äºadversarial robustness)
3. ICML 2018 Oral [Does Distributionally Robust Supervised Learning Give Robust Classifiers?](https://zhuanlan.zhihu.com/p/381176721)(DROå°±ä¸€å®šæ¯”ERMå¥½ï¼Ÿä¸ä¸€å®šï¼å¿…é¡»å¼•å…¥é¢å¤–ä¿¡æ¯)
4. NeurIPS 2019 [Distributionally Robust Optimization and Generalization in Kernel Methods](https://zhuanlan.zhihu.com/p/381176721)(æœ¬æ–‡ä½¿ç”¨MMD(maximummean discrepancy)å¯¹uncertainty setè¿›è¡Œå»ºæ¨¡ï¼Œå¾—åˆ°äº†MMD DRO)
5. EMNLP 2019 [Distributionally Robust Language Modeling](https://zhuanlan.zhihu.com/p/381176721)(Coarse-grained mixture modelsåœ¨NLPä¸­çš„ç»å…¸æ¡ˆä¾‹)
6. Arxiv 2019 [Equalizing recourse across groups](https://zhuanlan.zhihu.com/p/424631782)(åŸºç¡€çš„recourseæµ‹é‡çš„æ˜¯å•ä¸ªæ ·æœ¬ï¼Œæœ¬æ–‡ç»™å‡ºäº†ä¸€ä¸ªgroupçº§åˆ«çš„recourseåº¦é‡ã€‚)
7. ICML 2020 Oral [Continuously indexed domain adaptation](https://zhuanlan.zhihu.com/p/316265317)(è¿ç»­å˜åŒ–çš„domain)

# Causality

## Individual Treatment Estimation
1. ICML 2017 [Estimating individual treatment effect: generalization bounds and algorithms](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡ç¬¬ä¸€æ¬¡æå‡ºäº†ITEçš„æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨DAçš„ä¸€å¥—ç†è®ºå¯¹å…¶è¿›è¡Œboundï¼Œä¾æ¬¡è®¾è®¡äº†ä¸€å¥—è¡Œè€Œæœ‰æ•ˆçš„ç®—æ³•ã€‚)
2. NeurIPS 2019 [Adapting Neural Networks for the Estimation of Treatment Effects](https://zhuanlan.zhihu.com/p/426793887)(è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³æ˜¯è¿™æ ·çš„ï¼šæˆ‘ä»¬æ²¡å¿…è¦ä½¿ç”¨æ‰€æœ‰çš„åæ–¹å·®å˜é‡Xè¿›è¡Œadjustmentã€‚)
3. PNAS 2019 [Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶X-learnerï¼Œå½“å„ä¸ªtreatmentç»„çš„æ•°æ®éå¸¸ä¸å‡è¡¡çš„æ—¶å€™ï¼Œè¿™ç§æ¡†æ¶éå¸¸æœ‰æ•ˆã€‚)
4. AAAI 2020 [Learning Counterfactual Representations for Estimating Individual Dose-Response Curves](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡æå‡ºäº†æ–°çš„metricï¼Œæ–°çš„æ•°æ®é›†ï¼Œå’Œè®­ç»ƒç­–ç•¥ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„treatmentçš„outcomeè¿›è¡Œä¼°è®¡ã€‚)
5. ICLR 2021 Oral: [VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡åŸºäºvarying coefficient modelï¼Œè®©æ¯ä¸ªtreatmentå¯¹åº”çš„branchæˆä¸ºtreatmentçš„å‡½æ•°ï¼Œè€Œä¸éœ€è¦å•ç‹¬è®¾è®¡branchï¼Œä¾æ¬¡è¾¾åˆ°çœŸæ­£çš„è¿ç»­æ€§ã€‚)
6. Arxiv 2021 [Neural Counterfactual Representation Learning for Combinations of Treatments](https://zhuanlan.zhihu.com/p/426793887)(æœ¬æ–‡è€ƒè™‘æ›´å¤æ‚çš„æƒ…å†µï¼šå¤šç§treatmentå…±åŒä½œç”¨ã€‚)
7. NeurIPS 2021 Spotlight [On Inductive Biases for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2106.03765)(æœ¬æ–‡æå‡ºäº†æ–°æ¡†æ¶FlexTENetï¼Œç›´æ¥å¯¹æ¡ä»¶å› æœå€¼Ï„è¿›è¡Œä¼°è®¡ï¼Œè€Œä¸æ˜¯å¯¹Î¼1ï¼ŒÎ¼2åˆ†åˆ«ä¼°è®¡)
8. NeurIPS 2021 [Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory to Learning Algorithms](https://arxiv.org/abs/2101.10943)(æœ¬æ–‡åˆ†æäº†è¿›æ¥è¿›è¡Œ individual treatment effectçš„å„ç§ç®—æ³•èŒƒå¼ï¼Œ)
9. Arxiv 2021 [Cycle-Balanced Representation Learning For Counterfactual Inference](å¯¹treatmentï¼Œcontrolä¸¤ä¸ªgroupåˆ†åˆ«encodeï¼Œç„¶åå¯¹æŠ—å­¦ä¹ å‡å°‘åŸŸå·®è·ï¼Œä¸ºäº†é˜²æ­¢åˆ†ç±»ä¿¡æ¯è¢«æŠ¹å»ï¼ŒåŠ ä¸Šcycle-consistanceçš„çº¦æŸï¼Œé‡æ„ç‰¹å¾ã€‚)


# Data-Centric/Prompt

## Data Centric
1. AISTATS 2019 [Towards Optimal Transport with Global Invariances](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å¯¹é½ä¸¤ä¸ªæ•°æ®é›†ï¼Ÿ)
2. NeurIPS 2020 [Geometric Dataset Distances via Optimal Transport](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•å®šä¹‰ä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´çš„è·ç¦»ï¼Ÿ)
3. ICML 2021 [Dataset Dynamics via Gradient Flows in Probability Space](https://zhuanlan.zhihu.com/p/413791971)(å¦‚ä½•è¿›è¡Œæ•°æ®é›†ä¼˜åŒ–ï¼Œä½¿å¾—ä¸¤ä¸ªæ•°æ®é›†å°½å¯èƒ½çš„åƒï¼Ÿ)

## Prompts

1. ACL 2021 [WARP: Word-level Adversarial ReProgramming](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptå¼€ç¯‡ä¹‹ä½œ)
2. Arxiv 2021 **Stanford**[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://zhuanlan.zhihu.com/p/407144573)(Continuous Promptç”¨äºNLGçš„å„ç§ä»»åŠ¡)(å°†promptç”¨äºNLGä»»åŠ¡ä¸Š)
3. Arxiv 2021 **Google**[The Power of Scale for Parameter-Efficient Prompt Tuning](https://zhuanlan.zhihu.com/p/407144573)(ç›®å‰æœ€ç®€å•çš„preifx trainingï¼šåªå¯¹inputæ·»åŠ prefix)
4. Arxiv 2021 **DeepMind**[Multimodal Few-Shot Learning with Frozen Language Models](https://zhuanlan.zhihu.com/p/407144573)(åˆ©ç”¨å›¾åƒç¼–ç å™¨æŠŠå›¾åƒä½œä¸ºä¸€ç§åŠ¨æ€çš„prefixï¼Œä¸æ–‡æœ¬ä¸€èµ·é€å…¥LMä¸­)


# Optimization/GNN/Energy/Generative/Others

## Optimization
1. ICML 2021 [An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming](https://zhuanlan.zhihu.com/p/390808626)
2. NeurIPS 2021 _Deep Structural Causal Models for Tractable Counterfactual Inference_
1. ICML 2018 _Bilevel Programming for Hyperparameter Optimization and Meta-Learning_(ç”¨bi-level programmingå»ºæ¨¡è¶…å‚æ•°æœç´¢ä¸meta-learning)
2. NeurIPS 2021 [Energy-based Out-of-distribution Detection](https://zhuanlan.zhihu.com/p/343678039)

## LTH (Lottery Ticket Hypothesis)
1. NeurIPS 2020: [The Lottery Ticket Hypothesis for Pre-trained BERT Networks](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºBERT fine-tune))
2. ICML 2021 Oralï¼š [Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºOODæ³›åŒ–)
3. CVPR 2021: [The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models](https://zhuanlan.zhihu.com/p/404139792) (å½©ç¥¨å‡è®¾ç”¨äºè§†è§‰æ¨¡å‹é¢„è®­ç»ƒ)


## Generative Model (mainly diffusion model)
1. _Estimation of Non-Normalized Statistical Models by Score Matching_(ä½¿ç”¨åˆ†æ­¥ç§¯åˆ†ï¼ˆScore Matchingï¼‰çš„æ–¹æ³•è§£å†³éå½’ä¸€åŒ–åˆ†å¸ƒçš„ä¼°è®¡é—®é¢˜)
2. UAI 2019 _Sliced Score Matching: A Scalable Approach to Density and Score Estimation_(å°†é«˜ç»´çš„æ¢¯åº¦åœºæ²¿éšå³æ–¹å‘æŠ•å½±åˆ°ä¸€ç»´çš„æ ‡é‡åœºå†è¿›è¡Œscore-macthing) 
3. NeurIPS 2019 Oral _Generative Modeling by Estimating Gradients of the Data Distribution_(é€šè¿‡æ·»åŠ å™ªå£°çš„æ–¹æ³•ï¼Œå¢å¼ºLangevin MCMCå¯¹ä½æ¦‚ç‡å¯†åº¦åŒºåŸŸçš„å»ºæ¨¡èƒ½åŠ›)
4. NeurIPS 2020 _improved techniques for training score-based generative models_(å¯¹score-based generative modelå¤±è´¥æ¡ˆä¾‹çš„åˆ†æå’Œæ”¹è¿›ï¼Œç”Ÿæˆèƒ½åŠ›å¼€å§‹åª²ç¾GAN)
6. NeurIPS 2020 [Denoising Diffusion Probabilistic Models](https://zhuanlan.zhihu.com/p/366004028)(é™¤VAE, GAN, FLOWå¤–åˆä¸€ç”ŸæˆèŒƒå¼)
7. ICLR 2021 **Outstanding Paper Award** [Score-Based Generative Modeling through Stochastic Differential Equations](http://yang-song.github.io/blog/2021/score/)
8. Arxiv 2021 _Diffusion Models Beat GANs on Image Synthesis_(Diffusion Modelsåœ¨å›¾åƒå’Œåˆæˆä¸Šè¶…è¶ŠGAN) 
10. Arxiv 2021 Variational Diffusion Models

## Implicit Neural Representation (INR)
1. NeurIPS 2020 (Oral)ï¼š [Implicit Neural Representations with Periodic Activation Functions](https://zhuanlan.zhihu.com/p/472942119)
2. SIGGRAPH Asia 2020ï¼š [X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation](https://zhuanlan.zhihu.com/p/472942119)
3. CVPR 2021 (Oral)ï¼š[Learning Continuous Image Representation with Local Implicit Image Function](https://zhuanlan.zhihu.com/p/472942119)
4. CVPR 2021 [Adversarial Generation of Continuous Images](https://arxiv.org/abs/2011.12026)
5. NeurIPS 2021 [Learning Signal-Agnostic Manifolds of Neural Fields](https://arxiv.org/abs/2111.06387)
6. Arxiv 2021 [Generative Models as Distributions of Functions](https://arxiv.org/abs/2102.04776)


## Survey
1. [ç»¼è¿°ï¼šåŸºäºèƒ½é‡çš„æ¨¡å‹](https://zhuanlan.zhihu.com/p/343529491)
